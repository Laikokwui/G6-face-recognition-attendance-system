{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711e13bd",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "**All codes in Sections 0 and 1 need to be run before any models can be built.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b780870",
   "metadata": {},
   "source": [
    "## 0.1 Constants Declaration\n",
    "**Constants that represent absolute paths should be changed to match the folder and file locations of the inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6df75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_VERIFICATION_DIR = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_data\"\n",
    "FACE_VERIFICATION_TXT_PATH = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_pairs_val.txt\"\n",
    "IMAGE_SIZE = (256,256)\n",
    "IMAGES_PER_BATCH = 128\n",
    "CLASSES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04dd0",
   "metadata": {},
   "source": [
    "## 0.2 Simple ML Builder Class\n",
    "This class is created entirely by hand, to simplify the process of building and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0007d388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Loading TensorFlow... this will take a while.\n",
      "[SMLB] TensorFlow loaded! TensorFlow version is 2.7.0.\n",
      "[SMLB] Loading NumPy...\n",
      "[SMLB] NumPy loaded!\n",
      "[SMLB] Loading PyPlot...\n",
      "[SMLB] PyPlot loaded!\n",
      "[SMLB] All imports successful!\n",
      "[SMLB] Initialization successful!\n"
     ]
    }
   ],
   "source": [
    "import sys, math\n",
    "\n",
    "def smlb_log(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep)\n",
    "\n",
    "def smlb_log_error(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep, file=sys.stderr)\n",
    "\n",
    "smlb_log(\"Loading TensorFlow... this will take a while.\")\n",
    "import tensorflow as TensorFlow\n",
    "Keras = TensorFlow.keras\n",
    "smlb_log(\"TensorFlow loaded! TensorFlow version is\", TensorFlow.__version__ + \".\")\n",
    "\n",
    "smlb_log(\"Loading NumPy...\")\n",
    "import numpy as NumPy\n",
    "smlb_log(\"NumPy loaded!\")\n",
    "\n",
    "smlb_log(\"Loading PyPlot...\")\n",
    "from matplotlib import pyplot as PyPlot\n",
    "smlb_log(\"PyPlot loaded!\")\n",
    "\n",
    "smlb_log(\"All imports successful!\")\n",
    "\n",
    "class SimpleMLBuilder:\n",
    "\tdef __init__(self, verbose:bool=False):\n",
    "\t\tself.datasets = {\"training\": [None, None], \"validation\": None, \"testing\": [None, None]}\n",
    "\t\tself.layers = []\n",
    "\t\tself.labels = []\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.history = None\n",
    "\t\tself.log(\"Fully initialized!\")\n",
    "\t\n",
    "\tdef log(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log(*message, sep=sep)\n",
    "\t\n",
    "\tdef log_error(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log_error(*message, sep=sep)\n",
    "\t\n",
    "\tdef load_preset_dataset(self, preset:str):\n",
    "\t\t\"\"\"Loads a preset dataset with Keras.\n",
    "\t\t\n",
    "\t\tBuilt-in presets: MNIST & Fashion MNIST.\n",
    "\t\tUseful for testing the SMLB.\n",
    "\t\t\"\"\"\n",
    "\t\tpreset = preset.lower()\n",
    "\t\tif preset == \"mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.mnist.load_data()\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"MNIST\\\" loaded successfully.\")\n",
    "\t\telif preset == \"fashion mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"Fashion MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.fashion_mnist.load_data()\n",
    "\t\t\tself.labels = [\n",
    "\t\t\t\tNone,\n",
    "\t\t\t\t[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\t\t\t\t\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\t\t\t]\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset. Training images has shape of\", str(trainingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset. Testing images has shape of\", str(testingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"Fashion MNIST\\\" loaded successfully.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Preset \\\"\" + preset + \"\\\" not found.\")\n",
    "\t\n",
    "\tdef set_training_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for training.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][0] = features\n",
    "\t\tself.log(\"Training X values have been set!\")\n",
    "\tdef set_training_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for training.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the training features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][1] = labels\n",
    "\t\tself.log(\"Training Y values have been set!\")\n",
    "\tdef set_testing_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for testing.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][0] = features\n",
    "\t\tself.log(\"Testing X values have been set!\")\n",
    "\tdef set_testing_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for testing.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the testing features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][1] = labels\n",
    "\t\tself.log(\"Testing Y values have been set!\")\n",
    "\tdef set_validation_dataset(self, dataset):\n",
    "\t\t\"\"\"Sets the dataset used for validation.\n",
    "\t\t\n",
    "\t\tDataset should either be a TensorFlow.data.Dataset or a list of feature-label tuples.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"validation\"] = dataset\n",
    "\t\tself.log(\"Validation dataset has been set!\")\n",
    "\t\n",
    "\tdef get_training_features(self):\n",
    "\t\treturn self.datasets[\"training\"][0]\n",
    "\tdef get_training_labels(self):\n",
    "\t\treturn self.datasets[\"training\"][1]\n",
    "\tdef get_testing_features(self):\n",
    "\t\treturn self.datasets[\"testing\"][0]\n",
    "\tdef get_testing_labels(self):\n",
    "\t\treturn self.datasets[\"testing\"][1]\n",
    "\t\n",
    "\tdef get_feature_classes(self):\n",
    "\t\treturn self.labels[0]\n",
    "\tdef get_label_classes(self, y:bool=False):\n",
    "\t\treturn self.labels[1]\n",
    "\t\n",
    "\tdef start_layering(self, inputShape:tuple=None):\n",
    "\t\t\"\"\"Starts the creation of a new model.\n",
    "\t\t\n",
    "\t\tThis method also creates an input layer and adds it to the model.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.layers.clear()\n",
    "\t\tlayer = {\"type\": \"Input\", \"shape\": inputShape}\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Layering started. Added layer:\", layer)\n",
    "\tdef _add_layer(self, layer:dict):\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Added layer:\", layer)\n",
    "\t\n",
    "\tdef add_dense_layer(self, neurons:int):\n",
    "\t\t\"\"\"Adds a densely-connected layer to the model.\n",
    "\t\t\n",
    "\t\tDense layers are the bread and butter of any Deep Neural Network.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dense\", \"units\": neurons}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_activation_layer(self, activation:str=None):\n",
    "\t\t\"\"\"Adds an activation layer to the model.\n",
    "\t\t\n",
    "\t\tPossible activations: relu.\n",
    "\t\tIf there is a dense or convolution layer before this layer, that layer will be modified instead.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tlayerType = layer[\"type\"]\n",
    "\t\tif layerType == \"Dense\" or layerType == \"Conv2D\":\n",
    "\t\t\tlayer[\"activation\"] = activation\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself.log(\"Modified layer:\", layer)\n",
    "\t\telse:\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself._add_layer({\"type\": activation})\n",
    "\tdef add_rescaling_layer(self, scale:float=1.0/255, offset:float=0.0):\n",
    "\t\t\"\"\"Adds a rescaling layer to the model.\n",
    "\t\t\n",
    "\t\tUsed to add and multiply values of the previous layer.\n",
    "\t\tTypically used as a preprocessing layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Rescaling\", \"scale\": scale, \"offset\": offset}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_regularization_sublayer(self, regularization:str=None, regAmount:float=0.0):\n",
    "\t\t\"\"\"Modifies the previous layer to use regularization.\n",
    "\t\t\n",
    "\t\tPossible regularizations: l1, l2.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tif regularization==\"l1\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l1(regAmount)\n",
    "\t\telif regularization==\"l2\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l2(regAmount)\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Modified layer:\", layer)\n",
    "\tdef add_flatten_layer(self):\n",
    "\t\t\"\"\"Adds a flattening layer to the model.\n",
    "\t\t\n",
    "\t\tFlattening layers turn a n-dimensional input into a (n-1)-dimensional input,\n",
    "\t\twhere each vector in the tensor is concatenated with the last.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Flatten\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_dropout_layer(self, probability:float):\n",
    "\t\t\"\"\"Adds a dropout layer to the model.\n",
    "\t\t\n",
    "\t\tDropout layers have a chance to output 0 instead of the previous layer's values.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dropout\", \"rate\": probability}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_normalization_layer(self, axis:int=None):\n",
    "\t\t\"\"\"Normalizes input to be within a normal distribution of mean 0 and standard variance 1.\"\"\"\n",
    "\t\tlayer = {\"type\": \"Normalization\", \"axis\": axis}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_convolution_layer(self, filters:int, filterSize:tuple, stride:tuple=(1,1), pad:bool=False):\n",
    "\t\t\"\"\"Adds a convolution layer to the model.\n",
    "\t\t\n",
    "\t\tConvolution layers help to get certain image data features of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Conv2D\", \"filters\": filters, \"kernel_size\": filterSize, \"strides\": stride, \"padding\": \"same\" if pad else \"valid\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_pooling_layer(self, method:str=\"max\", filterSize:tuple=(1,1), stride:tuple=(1,1)):\n",
    "\t\t\"\"\"Adds a pooling layer to the model.\n",
    "\t\t\n",
    "\t\tPooling layers help to summarize image data of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tif method==\"max\":\n",
    "\t\t\tlayer = {\"type\": \"MaxPool2D\", \"pool_size\": filterSize, \"strides\": stride}\n",
    "\t\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef add_random_contrast_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Adds random image contrast to the input and outputs it.\n",
    "\t\t\n",
    "\t\tInput can be negative to reduce image contrast.\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\telse:\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomContrast\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_flip_layer(self, x:bool=False, y:bool=False):\n",
    "\t\t\"\"\"Has a 50% chance to flip the input around a given axis.\n",
    "\t\t\n",
    "\t\tx = allow horizontal flip, y = allow vertical flip\n",
    "\t\t\"\"\"\n",
    "\t\tflip = y and [x and \"horizontal_and_vertical\" or \"vertical\"] or \"horizontal\"\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomFlip\", \"mode\": flip}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_rotation_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly rotates the input around its center clockwise by the given amount of radians.\n",
    "\t\t\n",
    "\t\tInput can be negative to rotate counter-clockwise.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomRotation\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_zoom_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly zooms the input image by the given multiplier.\n",
    "\t\t\n",
    "\t\tInput can be negative to zoom out, up to > -1.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomZoom\", \"height_factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef get_layers(self) -> list:\n",
    "\t\treturn self.layers\n",
    "\t\n",
    "\tdef set_scc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.SparseCategoricalCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_bc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.BinaryCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_mae_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to the mean absolute error.\"\"\"\n",
    "\t\tself.lossFunction = \"mean_absolute_error\"\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_custom_loss_function(self, loss:Keras.losses):\n",
    "\t\t\"\"\"Sets the loss function to the passed value.\"\"\"\n",
    "\t\tself.lossFunction = loss\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\t\n",
    "\tdef _create_layer(self, layerData:dict) -> Keras.layers.Layer:\n",
    "\t\tlayerType = layerData.pop(\"type\")\n",
    "\t\tlayer = None\n",
    "\t\tif layerType == \"Input\":\n",
    "\t\t\tlayer = Keras.Input(**layerData)\n",
    "\t\telse:\n",
    "\t\t\tlayerCreationFunc = getattr(Keras.layers, layerType)\n",
    "\t\t\tlayer = layerCreationFunc(**layerData)\n",
    "\t\t\tif layerType == \"Normalization\":\n",
    "\t\t\t\tdataset = self.datasets[\"training\"][0]\n",
    "\t\t\t\tif dataset is TensorFlow.data.Dataset:\n",
    "\t\t\t\t\tlayer.adapt(dataset)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlayer.adapt(NumPy.array(dataset))\n",
    "\t\treturn layer\n",
    "\t\n",
    "\tdef build(self, learningRate:float=0.001, additionalMetrics:list=[]):\n",
    "\t\t\"\"\"Compiles the model based on the layers added.\"\"\"\n",
    "\t\tif len(self.layers)==0:\n",
    "\t\t\tself.log_error(\"Please add layers to the builder template before building.\")\n",
    "\t\telif not hasattr(self, \"lossFunction\"):\n",
    "\t\t\tself.log_error(\"Please specify the loss function first.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log(\"Building model with learning rate = \", learningRate, \"...\", sep=\"\")\n",
    "\t\t\t\n",
    "\t\t\tkerasLayers = []\n",
    "\t\t\tfor layer in self.layers:\n",
    "\t\t\t\tkerasLayers.append(self._create_layer(layer))\n",
    "\t\t\tmodel = Keras.models.Sequential(kerasLayers)\n",
    "\t\t\tif self.lossFunction == \"mean_absolute_error\":\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction,\n",
    "\t\t\t\t\tmetrics=[\"accuracy\"] + additionalMetrics\n",
    "\t\t\t\t)\n",
    "\t\t\tself.compiledModel = model\n",
    "\t\t\tself.log(\"Model built! Details:\")\n",
    "\t\t\tmodel.summary()\n",
    "\t\n",
    "\tdef destroy(self):\n",
    "\t\tdel self.compiledModel\n",
    "\t\tself.log(\"Model destroyed!\")\n",
    "\t\n",
    "\tdef get_model(self):\n",
    "\t\treturn self.compiledModel\n",
    "\t\n",
    "\tdef save(self, name:str=\"Unnamed\"):\n",
    "\t\tif hasattr(self, \"compiledModel\"):\n",
    "\t\t\tself.log(\"Saving model...\")\n",
    "\t\t\tself.compiledModel.save(name)\n",
    "\t\t\tself.log(\"Save complete!\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"No model to save!\")\n",
    "\t\n",
    "\tdef load(self, name:str):\n",
    "\t\tself.log(\"Loading model...\")\n",
    "\t\tself.compiledModel = Keras.models.load_model(name)\n",
    "\t\tself.log(\"Load complete!\")\n",
    "\t\n",
    "\tdef get_history(self):\n",
    "\t\treturn self.history\n",
    "\t\n",
    "\tdef _create_early_stopping_callback(self, epochs:int, hasValidation:bool=False) -> Keras.callbacks.EarlyStopping:\n",
    "\t\treturn Keras.callbacks.EarlyStopping(monitor=\"val_loss\" if hasValidation else \"loss\", mode=\"min\", patience=math.ceil(epochs ** 0.5), restore_best_weights=True)\n",
    "\t\n",
    "\tdef run(self, epochs:int, validationSplit:float=0.0, earlyStop:bool=False):\t\n",
    "\t\t\"\"\"Causes the model to start trying to fit to the training data.\n",
    "\t\t\n",
    "\t\tvalidationSplit is ignored if the validation dataset was specified via set_validation_dataset().\n",
    "\t\tearlyStop causes the model to stop training if the validation loss (or the training loss if no validation specified) does not improve after the square root amount of epochs, rounded up.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tfitArguments = {\n",
    "\t\t\t\t\"x\": self.datasets[\"training\"][0],\n",
    "\t\t\t\t\"y\": self.datasets[\"training\"][1],\n",
    "\t\t\t\t\"epochs\": epochs,\n",
    "\t\t\t\t\"validation_data\": self.datasets[\"validation\"],\n",
    "\t\t\t\t\"validation_split\": validationSplit\n",
    "\t\t\t}\n",
    "\t\t\tif earlyStop:\n",
    "\t\t\t\tfitArguments[\"callbacks\"] = [self._create_early_stopping_callback(epochs, True if self.datasets[\"validation\"] else validation_split > 0.0)]\n",
    "\t\t\tself.history = self.compiledModel.fit(**fitArguments)\n",
    "\t\t\tself.log(\"======== TRAINING DONE ========\", nonVerbose=True)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef plot(self):\n",
    "\t\t\"\"\"Plots the training progress via MatPlotLib.\"\"\"\n",
    "\t\tif self.history:\n",
    "\t\t\tself.log(\"Creating graphs, please wait...\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\thistoryDict = self.history.history\n",
    "\t\t\t\n",
    "\t\t\tepochsRange = range(self.history.params[\"epochs\"])\n",
    "\t\t\taccuracy = historyDict[\"accuracy\"]\n",
    "\t\t\tloss = historyDict[\"loss\"]\n",
    "\t\t\t\n",
    "\t\t\tif \"val_accuracy\" in historyDict:\n",
    "\t\t\t\tvalidationAccuracy = historyDict[\"val_accuracy\"]\n",
    "\t\t\t\tvalidationLoss = historyDict[\"val_loss\"]\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.figure(figsize=(12, 6))\n",
    "\t\t\tPyPlot.subplot(1, 2, 1)\n",
    "\t\t\tPyPlot.plot(epochsRange, accuracy, label=\"Training Accuracy\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationAccuracy, label=\"Validation Accuracy\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Accuracy\")\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.subplot(1, 2, 2)\n",
    "\t\t\tPyPlot.plot(epochsRange, loss, label=\"Training Loss\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationLoss, label=\"Validation Loss\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Loss\")\n",
    "\t\t\tPyPlot.show()\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please run the model first.\")\n",
    "\t\n",
    "\tdef evaluate(self) -> tuple:\n",
    "\t\t\"\"\"Evaluates the model over the given training dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tresults = self.compiledModel.evaluate(self.datasets[\"testing\"][0], self.datasets[\"testing\"][1], verbose=2)\n",
    "\t\t\tself.log(\"======== TESTING DONE ========\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\treturn results\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef predict(self, features) -> NumPy.ndarray:\n",
    "\t\t\"\"\"Makes the model do predictions over the given testing dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tpredictionModel = Keras.Sequential([self.compiledModel, Keras.layers.Softmax()])\n",
    "\t\t\treturn predictionModel.predict(features)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build and train the model first.\")\n",
    "\n",
    "smlb_log(\"Initialization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673482f7",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation\n",
    "`verification_data.zip` is a large collection of face images. We can't use `tf.keras.utils.image_dataset_from_directory` here since the model needs two inputs, so we have to build the dataset from scratch again.\n",
    "## 1.1 Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553949b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the classification image folder into a tf.data.Dataset object\n",
    "Keras.utils.image_dataset_from_directory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
