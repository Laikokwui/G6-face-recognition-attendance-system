{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711e13bd",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "**All codes in Sections 0 and 1 need to be run before any models can be built.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b780870",
   "metadata": {},
   "source": [
    "## 0.1 Constants Declaration\n",
    "**Constants that represent absolute paths should be changed to match the folder and file locations of the inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6df75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\classification\\train\"\n",
    "IMAGE_SIZE = (64,64)\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#DIR_PATH = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_data\"\n",
    "#TXT_PATH = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_pairs_val.txt\"\n",
    "#TRAINING_PAIRS = 4096\n",
    "#TESTING_PAIRS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04dd0",
   "metadata": {},
   "source": [
    "## 0.2 Simple ML Builder Class\n",
    "This class is created entirely by hand, to simplify the process of building and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0007d388",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Loading compulsory modules...\n",
      "[SMLB] Loading TensorFlow... this will take a while.\n",
      "[SMLB] TensorFlow loaded! TensorFlow version is 2.7.0.\n",
      "[SMLB] Loading NumPy...\n",
      "[SMLB] NumPy loaded!\n",
      "[SMLB] Loading PyPlot...\n",
      "[SMLB] PyPlot loaded!\n",
      "[SMLB] All compulsory imports successful!\n",
      "[SMLB] Loading optional modules...\n",
      "[SMLB] Loading TensorFlow Addons...\n",
      "[SMLB] TensorFlow Addons loaded! TensorFlow Addons version is 0.15.0.\n",
      "[SMLB] Loading SciKitLearn...\n",
      "[SMLB] SciKitLearn loaded!\n",
      "[SMLB] Optional module imports processed.\n",
      "[SMLB] Initialization successful!\n"
     ]
    }
   ],
   "source": [
    "import sys, math\n",
    "\n",
    "def smlb_log(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep)\n",
    "\n",
    "def smlb_log_error(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep, file=sys.stderr)\n",
    "\n",
    "smlb_log(\"Loading compulsory modules...\")\n",
    "\n",
    "smlb_log(\"Loading TensorFlow... this will take a while.\")\n",
    "import tensorflow as TensorFlow\n",
    "Keras = TensorFlow.keras\n",
    "smlb_log(\"TensorFlow loaded! TensorFlow version is\", TensorFlow.__version__ + \".\")\n",
    "\n",
    "smlb_log(\"Loading NumPy...\")\n",
    "import numpy as NumPy\n",
    "smlb_log(\"NumPy loaded!\")\n",
    "\n",
    "smlb_log(\"Loading PyPlot...\")\n",
    "from matplotlib import pyplot as PyPlot\n",
    "smlb_log(\"PyPlot loaded!\")\n",
    "\n",
    "smlb_log(\"All compulsory imports successful!\")\n",
    "smlb_log(\"Loading optional modules...\")\n",
    "\n",
    "smlb_log(\"Loading TensorFlow Addons...\")\n",
    "try:\n",
    "\timport tensorflow_addons as TensorFlowAddons\n",
    "except ImportError:\n",
    "\tsmlb_log(\"TensorFlow Addons not installed.\")\n",
    "else:\n",
    "\tsmlb_log(\"TensorFlow Addons loaded! TensorFlow Addons version is\", TensorFlowAddons.__version__ + \".\")\n",
    "\n",
    "smlb_log(\"Loading SciKitLearn...\")\n",
    "try:\n",
    "\timport sklearn\n",
    "except ImportError:\n",
    "\tsmlb_log(\"SciKitLearn not installed.\")\n",
    "else:\n",
    "\tsmlb_log(\"SciKitLearn loaded!\")\n",
    "\t\n",
    "smlb_log(\"Optional module imports processed.\")\n",
    "\n",
    "class SimpleMLBuilder:\n",
    "\tdef __init__(self, verbose:bool=False):\n",
    "\t\tself.datasets = {\"training\": [None, None], \"validation\": None, \"testing\": [None, None]}\n",
    "\t\tself.layers = []\n",
    "\t\tself.labels = []\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.history = None\n",
    "\t\tself.log(\"Fully initialized!\")\n",
    "\t\n",
    "\tdef log(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log(*message, sep=sep)\n",
    "\t\n",
    "\tdef log_error(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log_error(*message, sep=sep)\n",
    "\t\n",
    "\tdef load_preset_dataset(self, preset:str):\n",
    "\t\t\"\"\"Loads a preset dataset with Keras.\n",
    "\t\t\n",
    "\t\tBuilt-in presets: MNIST & Fashion MNIST.\n",
    "\t\tUseful for testing the SMLB.\n",
    "\t\t\"\"\"\n",
    "\t\tpreset = preset.lower()\n",
    "\t\tif preset == \"mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.mnist.load_data()\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"MNIST\\\" loaded successfully.\")\n",
    "\t\telif preset == \"fashion mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"Fashion MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.fashion_mnist.load_data()\n",
    "\t\t\tself.labels = [\n",
    "\t\t\t\tNone,\n",
    "\t\t\t\t[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\t\t\t\t\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\t\t\t]\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset. Training images has shape of\", str(trainingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset. Testing images has shape of\", str(testingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"Fashion MNIST\\\" loaded successfully.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Preset \\\"\" + preset + \"\\\" not found.\")\n",
    "\t\n",
    "\tdef set_training_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for training.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][0] = features\n",
    "\t\tself.log(\"Training X values have been set!\")\n",
    "\tdef set_training_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for training.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the training features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][1] = labels\n",
    "\t\tself.log(\"Training Y values have been set!\")\n",
    "\tdef set_testing_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for testing.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][0] = features\n",
    "\t\tself.log(\"Testing X values have been set!\")\n",
    "\tdef set_testing_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for testing.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the testing features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][1] = labels\n",
    "\t\tself.log(\"Testing Y values have been set!\")\n",
    "\tdef set_validation_dataset(self, dataset):\n",
    "\t\t\"\"\"Sets the dataset used for validation.\n",
    "\t\t\n",
    "\t\tDataset should either be a TensorFlow.data.Dataset or a list of feature-label tuples.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"validation\"] = dataset\n",
    "\t\tself.log(\"Validation dataset has been set!\")\n",
    "\t\n",
    "\tdef get_training_features(self):\n",
    "\t\treturn self.datasets[\"training\"][0]\n",
    "\tdef get_training_labels(self):\n",
    "\t\treturn self.datasets[\"training\"][1]\n",
    "\tdef get_testing_features(self):\n",
    "\t\treturn self.datasets[\"testing\"][0]\n",
    "\tdef get_testing_labels(self):\n",
    "\t\treturn self.datasets[\"testing\"][1]\n",
    "\t\n",
    "\tdef get_feature_classes(self):\n",
    "\t\treturn self.labels[0]\n",
    "\tdef get_label_classes(self, y:bool=False):\n",
    "\t\treturn self.labels[1]\n",
    "\t\n",
    "\tdef start_layering(self, inputShape:tuple=None):\n",
    "\t\t\"\"\"Starts the creation of a new model.\n",
    "\t\t\n",
    "\t\tThis method also creates an input layer and adds it to the model.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.layers.clear()\n",
    "\t\tlayer = {\"type\": \"Input\", \"shape\": inputShape}\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Layering started. Added layer:\", layer)\n",
    "\t\n",
    "\tdef _add_layer(self, layer:dict):\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Added layer:\", layer)\n",
    "\tdef add_dense_layer(self, neurons:int):\n",
    "\t\t\"\"\"Adds a densely-connected layer to the model.\n",
    "\t\t\n",
    "\t\tDense layers are the bread and butter of any Deep Neural Network.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dense\", \"units\": neurons}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_activation_layer(self, activation:str=None):\n",
    "\t\t\"\"\"Adds an activation layer to the model.\n",
    "\t\t\n",
    "\t\tPossible activations: relu.\n",
    "\t\tIf there is a dense or convolution layer before this layer, that layer will be modified instead.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tlayerType = layer[\"type\"]\n",
    "\t\tif layerType == \"Dense\" or layerType == \"Conv2D\":\n",
    "\t\t\tlayer[\"activation\"] = activation\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself.log(\"Modified layer:\", layer)\n",
    "\t\telse:\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself._add_layer({\"type\": activation})\n",
    "\tdef add_rescaling_layer(self, scale:float=1.0/255, offset:float=0.0):\n",
    "\t\t\"\"\"Adds a rescaling layer to the model.\n",
    "\t\t\n",
    "\t\tUsed to add and multiply values of the previous layer.\n",
    "\t\tTypically used as a preprocessing layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Rescaling\", \"scale\": scale, \"offset\": offset}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_regularization_sublayer(self, regularization:str=None, regAmount:float=0.0):\n",
    "\t\t\"\"\"Modifies the previous layer to use regularization.\n",
    "\t\t\n",
    "\t\tPossible regularizations: l1, l2.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tif regularization==\"l1\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l1(regAmount)\n",
    "\t\telif regularization==\"l2\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l2(regAmount)\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Modified layer:\", layer)\n",
    "\tdef add_regularization_layer(self, regularization:str=None, regAmount:float=0.0):\n",
    "\t\t\"\"\"Adds a regularization layer to the model, as a lambda layer.\n",
    "\t\t\n",
    "\t\tIf this layer is being attached to the previous layer, consider using add_regularization_sublayer instead.\n",
    "\t\tPossible regularizations: l2.\n",
    "\t\t\"\"\"\n",
    "\t\tif regularization==\"l2\":\n",
    "\t\t\tlayer = {\"type\": \"Lambda\", \"function\": lambda x: TensorFlow.math.l2_normalize(x, axis=1)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_flatten_layer(self):\n",
    "\t\t\"\"\"Adds a flattening layer to the model.\n",
    "\t\t\n",
    "\t\tFlattening layers turn a n-dimensional input into a (n-1)-dimensional input,\n",
    "\t\twhere each vector in the tensor is concatenated with the last.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Flatten\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_softmax_layer(self):\n",
    "\t\t\"\"\"Adds a softmax layer to the model.\n",
    "\t\t\n",
    "\t\tSoftmax layers turn a tensor of logistic values into a tensor of probablistic values.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Softmax\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_dropout_layer(self, probability:float):\n",
    "\t\t\"\"\"Adds a dropout layer to the model.\n",
    "\t\t\n",
    "\t\tDropout layers have a chance to output 0 instead of the previous layer's values.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dropout\", \"rate\": probability}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_normalization_layer(self, axis:int=None):\n",
    "\t\t\"\"\"Normalizes input to be within a normal distribution of mean 0 and standard variance 1.\"\"\"\n",
    "\t\tlayer = {\"type\": \"Normalization\", \"axis\": axis}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_convolution_layer(self, filters:int, filterSize:tuple, stride:tuple=(1,1), pad:bool=False):\n",
    "\t\t\"\"\"Adds a convolution layer to the model.\n",
    "\t\t\n",
    "\t\tConvolution layers help to get certain image data features of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Conv2D\", \"filters\": filters, \"kernel_size\": filterSize, \"strides\": stride, \"padding\": \"same\" if pad else \"valid\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_pooling_layer(self, method:str=\"max\", filterSize:tuple=(1,1), stride:tuple=(1,1)):\n",
    "\t\t\"\"\"Adds a pooling layer to the model.\n",
    "\t\t\n",
    "\t\tPooling layers help to summarize image data of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tif method==\"max\":\n",
    "\t\t\tlayer = {\"type\": \"MaxPool2D\", \"pool_size\": filterSize, \"strides\": stride}\n",
    "\t\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef add_random_contrast_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Adds random image contrast to the input and outputs it.\n",
    "\t\t\n",
    "\t\tInput can be negative to reduce image contrast.\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\telse:\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomContrast\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_flip_layer(self, x:bool=False, y:bool=False):\n",
    "\t\t\"\"\"Has a 50% chance to flip the input around a given axis.\n",
    "\t\t\n",
    "\t\tx = allow horizontal flip, y = allow vertical flip\n",
    "\t\t\"\"\"\n",
    "\t\tflip = y and [x and \"horizontal_and_vertical\" or \"vertical\"] or \"horizontal\"\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomFlip\", \"mode\": flip}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_rotation_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly rotates the input around its center clockwise by the given amount of radians.\n",
    "\t\t\n",
    "\t\tInput can be negative to rotate counter-clockwise.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomRotation\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_zoom_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly zooms the input image by the given multiplier.\n",
    "\t\t\n",
    "\t\tInput can be negative to zoom out, up to > -1.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomZoom\", \"height_factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef get_layers(self) -> list:\n",
    "\t\treturn self.layers\n",
    "\t\n",
    "\tdef set_scc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.SparseCategoricalCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_bc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.BinaryCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_mae_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to the mean absolute error.\"\"\"\n",
    "\t\tself.lossFunction = \"mean_absolute_error\"\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_custom_loss_function(self, loss:Keras.losses):\n",
    "\t\t\"\"\"Sets the loss function to the passed value.\"\"\"\n",
    "\t\tself.lossFunction = loss\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\t\n",
    "\tdef _create_layer(self, layerData:dict) -> Keras.layers.Layer:\n",
    "\t\tlayerType = layerData.pop(\"type\")\n",
    "\t\tlayer = None\n",
    "\t\tif layerType == \"Input\":\n",
    "\t\t\tlayer = Keras.Input(**layerData)\n",
    "\t\telse:\n",
    "\t\t\tlayerCreationFunc = getattr(Keras.layers, layerType)\n",
    "\t\t\tlayer = layerCreationFunc(**layerData)\n",
    "\t\t\tif layerType == \"Normalization\":\n",
    "\t\t\t\tdataset = self.datasets[\"training\"][0]\n",
    "\t\t\t\tif dataset is TensorFlow.data.Dataset:\n",
    "\t\t\t\t\tlayer.adapt(dataset)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlayer.adapt(NumPy.array(dataset))\n",
    "\t\treturn layer\n",
    "\tdef build(self, learningRate:float=0.001, additionalMetrics:list=[]):\n",
    "\t\t\"\"\"Builds and compiles the model based on the layers added.\"\"\"\n",
    "\t\tif len(self.layers)==0:\n",
    "\t\t\tself.log_error(\"Please add layers to the builder template before building.\")\n",
    "\t\telif not hasattr(self, \"lossFunction\"):\n",
    "\t\t\tself.log_error(\"Please specify the loss function first.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log(\" Building model with learning rate = \", learningRate, \"...\", sep=\"\")\n",
    "\t\t\t\n",
    "\t\t\tkerasLayers = []\n",
    "\t\t\tfor layer in self.layers:\n",
    "\t\t\t\tkerasLayers.append(self._create_layer(layer))\n",
    "\t\t\tmodel = Keras.models.Sequential(kerasLayers)\n",
    "\t\t\tif self.lossFunction is not Keras.losses.SparseCategoricalCrossentropy:\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction,\n",
    "\t\t\t\t\tmetrics=[\"accuracy\"] + additionalMetrics\n",
    "\t\t\t\t)\n",
    "\t\t\tself.compiledModel = model\n",
    "\t\t\tself.log(\"Model built! Details:\")\n",
    "\t\t\tmodel.summary()\n",
    "\tdef build_and_eject_before_compilation(self) -> Keras.Model:\n",
    "\t\t\"\"\"Builds the model based on the layers added, but ejects the model before compilation.\"\"\"\n",
    "\t\tif len(self.layers)==0:\n",
    "\t\t\tself.log_error(\"Please add layers to the builder template before building.\")\n",
    "\t\telse:\n",
    "\t\t\tkerasLayers = []\n",
    "\t\t\tfor layer in self.layers:\n",
    "\t\t\t\tkerasLayers.append(self._create_layer(layer))\n",
    "\t\t\treturn Keras.models.Sequential(kerasLayers)\n",
    "\t\n",
    "\tdef destroy(self):\n",
    "\t\tdel self.compiledModel\n",
    "\t\tself.log(\"Model destroyed!\")\n",
    "\t\n",
    "\tdef set_compiled_model(self, model:Keras.Model):\n",
    "\t\tself.compiledModel = model\n",
    "\tdef get_compiled_model(self) -> Keras.Model:\n",
    "\t\treturn self.compiledModel\n",
    "\t\n",
    "\tdef save(self, name:str=\"Unnamed\"):\n",
    "\t\tif hasattr(self, \"compiledModel\"):\n",
    "\t\t\tself.log(\"Saving model...\")\n",
    "\t\t\tself.compiledModel.save(name)\n",
    "\t\t\tself.log(\"Save complete!\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"No model to save!\")\n",
    "\tdef load(self, name:str):\n",
    "\t\tself.log(\"Loading model...\")\n",
    "\t\tself.compiledModel = Keras.models.load_model(name)\n",
    "\t\tself.log(\"Load complete!\")\n",
    "\t\n",
    "\tdef get_history(self):\n",
    "\t\treturn self.history\n",
    "\t\n",
    "\tdef _create_early_stopping_callback(self, epochs:int, hasValidation:bool=False) -> Keras.callbacks.EarlyStopping:\n",
    "\t\treturn Keras.callbacks.EarlyStopping(monitor=\"val_loss\" if hasValidation else \"loss\", mode=\"min\", patience=math.ceil(epochs ** 0.5), restore_best_weights=True)\n",
    "\t\n",
    "\tdef run(self, epochs:int, validationSplit:float=0.0, earlyStop:bool=False):\t\n",
    "\t\t\"\"\"Causes the model to start trying to fit to the training data.\n",
    "\t\t\n",
    "\t\tvalidationSplit is ignored if the validation dataset was specified via set_validation_dataset().\n",
    "\t\tearlyStop causes the model to stop training if the validation loss (or the training loss if no validation specified) does not improve after the square root amount of epochs, rounded up.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tfitArguments = {\n",
    "\t\t\t\t\"x\": self.datasets[\"training\"][0],\n",
    "\t\t\t\t\"y\": self.datasets[\"training\"][1],\n",
    "\t\t\t\t\"epochs\": epochs,\n",
    "\t\t\t\t\"validation_data\": self.datasets[\"validation\"],\n",
    "\t\t\t\t\"validation_split\": validationSplit\n",
    "\t\t\t}\n",
    "\t\t\tif earlyStop:\n",
    "\t\t\t\tfitArguments[\"callbacks\"] = [self._create_early_stopping_callback(epochs, True if self.datasets[\"validation\"] else validation_split > 0.0)]\n",
    "\t\t\tself.history = self.compiledModel.fit(**fitArguments)\n",
    "\t\t\tself.log(\"======== TRAINING DONE ========\", nonVerbose=True)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef plot(self):\n",
    "\t\t\"\"\"Plots the training progress via MatPlotLib.\"\"\"\n",
    "\t\tif self.history:\n",
    "\t\t\tself.log(\"Creating graphs, please wait...\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\thistoryDict = self.history.history\n",
    "\t\t\t\n",
    "\t\t\tepochsRange = range(self.history.params[\"epochs\"])\n",
    "\t\t\taccuracy = historyDict[\"accuracy\"]\n",
    "\t\t\tloss = historyDict[\"loss\"]\n",
    "\t\t\t\n",
    "\t\t\tif \"val_accuracy\" in historyDict:\n",
    "\t\t\t\tvalidationAccuracy = historyDict[\"val_accuracy\"]\n",
    "\t\t\t\tvalidationLoss = historyDict[\"val_loss\"]\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.figure(figsize=(12, 6))\n",
    "\t\t\tPyPlot.subplot(1, 2, 1)\n",
    "\t\t\tPyPlot.plot(epochsRange, accuracy, label=\"Training Accuracy\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationAccuracy, label=\"Validation Accuracy\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Accuracy\")\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.subplot(1, 2, 2)\n",
    "\t\t\tPyPlot.plot(epochsRange, loss, label=\"Training Loss\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationLoss, label=\"Validation Loss\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Loss\")\n",
    "\t\t\tPyPlot.show()\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please run the model first.\")\n",
    "\t\n",
    "\tdef evaluate(self) -> tuple:\n",
    "\t\t\"\"\"Evaluates the model over the given training dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tresults = self.compiledModel.evaluate(self.datasets[\"testing\"][0], self.datasets[\"testing\"][1], verbose=2)\n",
    "\t\t\tself.log(\"======== TESTING DONE ========\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\treturn results\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef predict(self, features) -> NumPy.ndarray:\n",
    "\t\t\"\"\"Makes the model do predictions over the given testing dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tpredictionModel = Keras.Sequential([self.compiledModel, Keras.layers.Softmax()])\n",
    "\t\t\treturn predictionModel.predict(features)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build and train the model first.\")\n",
    "\n",
    "smlb_log(\"Initialization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673482f7",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation\n",
    "## 1.1 Training and Validation Datasets\n",
    "### 1.1.1 Creation\n",
    "We can use `tf.keras.utils.image_dataset_from_directory` to create the `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae2e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 200 classes.\n",
      "Using 4500 files for training.\n",
      "Found 5000 files belonging to 200 classes.\n",
      "Using 500 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed = random.randrange(0, 16777216)\n",
    "trainingDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"training\", interpolation=\"bicubic\"\n",
    ")\n",
    "validationDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"validation\", interpolation=\"bicubic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a68bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(45, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Test sample\n",
    "print(trainingDataset.cardinality())\n",
    "print(validationDataset.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6267",
   "metadata": {},
   "source": [
    "### 1.1.2 Optimization\n",
    "The datasets need to be optimized to make sure disk I/O does not slow down training unecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b33c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingOptimizedDataset = trainingDataset.cache().shuffle(1000).prefetch(buffer_size=TensorFlow.data.AUTOTUNE)\n",
    "validationOptimizedDataset = validationDataset.cache().prefetch(buffer_size=TensorFlow.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c33ad",
   "metadata": {},
   "source": [
    "The final datasets, `trainingOptimizedDataset` and `testingOptimizedDataset`, can now be used for training the model.\n",
    "# 2. Model Building\n",
    "The models that need to be built are a cascading series of layers with triplet loss.\n",
    "## 2.1. Basic Model\n",
    "The following is the first architecture used:\n",
    "- Input Rescaler (factor=1/255)\n",
    "- Padded 3x3 Convolution Stride 1x1 (x64) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x64) with ReLU activation\n",
    "- 2x2 Max Pooling Stride 2x2\n",
    "- Padded 3x3 Convolution Stride 1x1 (x128) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x128) with ReLU activation\n",
    "- 2x2 Max Pooling Stride 2x2\n",
    "- Padded 3x3 Convolution Stride 1x1 (x256) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x256) with ReLU activation\n",
    "- Axis Flattener\n",
    "- Fully Connected (x256)\n",
    "- Softmax as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb88f6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Fully initialized!\n",
      "[SMLB] Training X values have been set!\n",
      "[SMLB] Validation dataset has been set!\n",
      "[SMLB] Layering started. Added layer: {'type': 'Input', 'shape': (64, 64, 3)}\n",
      "[SMLB] Added layer: {'type': 'Rescaling', 'scale': 0.00392156862745098, 'offset': 0.0}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Flatten'}\n",
      "[SMLB] Added layer: {'type': 'Dense', 'units': 256}\n",
      "[SMLB] Added layer: {'type': 'Lambda', 'function': <function SimpleMLBuilder.add_regularization_layer.<locals>.<lambda> at 0x00000237F56803A0>}\n",
      "[SMLB] Set loss function: <tensorflow_addons.losses.triplet.TripletSemiHardLoss object at 0x00000237F56846D0>\n",
      "[SMLB] Building model with learning rate = 0.001...\n",
      "[SMLB] Model built! Details:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 65536)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16777472  \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 256)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,922,880\n",
      "Trainable params: 17,922,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smlb = SimpleMLBuilder(True)\n",
    "smlb.set_training_features(trainingOptimizedDataset)\n",
    "smlb.set_validation_dataset(validationOptimizedDataset)\n",
    "\n",
    "smlb.start_layering((64, 64, 3))\n",
    "smlb.add_rescaling_layer()\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_flatten_layer()\n",
    "smlb.add_dense_layer(256)\n",
    "smlb.add_regularization_layer(\"l2\")\n",
    "\n",
    "smlb.set_custom_loss_function(TensorFlowAddons.losses.TripletSemiHardLoss())\n",
    "smlb.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81b32fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 20s 168ms/step - loss: 0.9918 - val_loss: 0.9819\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9855 - val_loss: 0.9820\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9837 - val_loss: 0.9783\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9806 - val_loss: 0.9841\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9835 - val_loss: 0.9797\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9780 - val_loss: 0.9777\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9733 - val_loss: 0.9710\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9727 - val_loss: 0.9714\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 7s 157ms/step - loss: 0.9674 - val_loss: 0.9749\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 7s 156ms/step - loss: 0.9582 - val_loss: 0.9753\n",
      "[SMLB] ======== TRAINING DONE ========\n"
     ]
    }
   ],
   "source": [
    "smlb.run(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2295e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 1.08529434e+01,  2.94117928e-02, -6.48789033e-02],\n",
      "        [ 1.33561621e+01,  2.08608866e+00,  1.20223463e-02],\n",
      "        [ 1.86526737e+01,  6.43893242e+00,  1.72429323e-01],\n",
      "        ...,\n",
      "        [ 5.04021149e+01,  1.63185921e+01,  4.71486263e-02],\n",
      "        [ 4.97640686e+01,  1.79547024e+01, -5.02361581e-02],\n",
      "        [ 4.94628143e+01,  1.87275105e+01, -9.60207731e-02]],\n",
      "\n",
      "       [[ 1.16309586e+01,  7.59768188e-01, -1.72606260e-01],\n",
      "        [ 1.40841818e+01,  2.77910423e+00,  9.73946303e-02],\n",
      "        [ 1.92744198e+01,  7.05248833e+00,  6.66629434e-01],\n",
      "        ...,\n",
      "        [ 5.08459320e+01,  1.71460419e+01, -2.90132046e-01],\n",
      "        [ 5.09118233e+01,  1.94439774e+01,  8.19969177e-02],\n",
      "        [ 5.09418259e+01,  2.05283413e+01,  2.57406622e-01]],\n",
      "\n",
      "       [[ 1.32777290e+01,  2.30467010e+00, -4.01436925e-01],\n",
      "        [ 1.56245623e+01,  4.24466515e+00,  2.77037948e-01],\n",
      "        [ 2.05886593e+01,  8.34922409e+00,  1.71108890e+00],\n",
      "        ...,\n",
      "        [ 5.17799683e+01,  1.88935394e+01, -1.00815797e+00],\n",
      "        [ 5.33417282e+01,  2.25980949e+01,  3.61564577e-01],\n",
      "        [ 5.40756683e+01,  2.43446369e+01,  1.00696015e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.71334778e+02,  1.35407959e+02,  1.08225647e+02],\n",
      "        [ 1.74961212e+02,  1.38582672e+02,  1.10839478e+02],\n",
      "        [ 1.82643387e+02,  1.45308380e+02,  1.16377136e+02],\n",
      "        ...,\n",
      "        [ 1.61631603e+01,  1.38949947e+01,  1.50171900e+01],\n",
      "        [ 1.57325459e+01,  1.25808220e+01,  1.34713326e+01],\n",
      "        [ 1.55303116e+01,  1.19609346e+01,  1.27427044e+01]],\n",
      "\n",
      "       [[ 1.73019974e+02,  1.37831055e+02,  1.10153519e+02],\n",
      "        [ 1.74716125e+02,  1.39480469e+02,  1.11229164e+02],\n",
      "        [ 1.78307983e+02,  1.42974701e+02,  1.13507782e+02],\n",
      "        ...,\n",
      "        [ 1.60093613e+01,  1.37545004e+01,  1.49846764e+01],\n",
      "        [ 1.48069696e+01,  1.24722691e+01,  1.34983215e+01],\n",
      "        [ 1.42404480e+01,  1.18673258e+01,  1.27977676e+01]],\n",
      "\n",
      "       [[ 1.73813171e+02,  1.38974075e+02,  1.11062309e+02],\n",
      "        [ 1.74598770e+02,  1.39904053e+02,  1.11412430e+02],\n",
      "        [ 1.76261139e+02,  1.41874054e+02,  1.12153809e+02],\n",
      "        ...,\n",
      "        [ 1.59364643e+01,  1.36885967e+01,  1.49694672e+01],\n",
      "        [ 1.43699026e+01,  1.24214268e+01,  1.35109501e+01],\n",
      "        [ 1.36314898e+01,  1.18235312e+01,  1.28235331e+01]]],\n",
      "      dtype=float32), 134)\n"
     ]
    }
   ],
   "source": [
    "# Test sample\n",
    "for data in trainingOptimizedDataset.unbatch().take(1).as_numpy_iterator():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8b1e9",
   "metadata": {},
   "source": [
    "## 1.2 Testing Dataset (TODO)\n",
    "`verification_data` contains a large collection of face images. We can't use `tf.keras.utils.image_dataset_from_directory` here since the model needs two inputs, so we have to build the datasets from scratch again.\n",
    "First, we need to gather all of the file paths and parity labels as indicated by `FACE_VERIFICATION_TXT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314ec7e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TXT_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-be87459e2349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstringList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTXT_PATH\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfileHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileHandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TXT_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import io\n",
    "stringList = []\n",
    "with open(TXT_PATH) as fileHandle:\n",
    "    i = 0\n",
    "    for line in fileHandle:\n",
    "        stringList.append(line.split())\n",
    "        i += 1\n",
    "        if (i >= TRAINING_PAIRS + TESTING_PAIRS):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stringList), stringList[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c571e6b",
   "metadata": {},
   "source": [
    "After reading from the file, we need to turn `stringList` into a `tf.data.Dataset`. This is not trivial.\n",
    "\n",
    "The first step is to turn the above list into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbefb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringTensor = TensorFlow.constant(stringList, TensorFlow.dtypes.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stringTensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc46f1",
   "metadata": {},
   "source": [
    "Now we need a function that returns the image data as a tensor of floats and the label as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stringZeroTensor = TensorFlow.constant('0', TensorFlow.dtypes.string)\n",
    "floatZeroTensor = TensorFlow.constant(0, TensorFlow.dtypes.float32)\n",
    "floatOneTensor = TensorFlow.constant(1, TensorFlow.dtypes.float32)\n",
    "\n",
    "def getImageTensorByAbsolutePath(absolutePath:TensorFlow.Tensor)\\\n",
    "-> TensorFlow.Tensor:\n",
    "    # Load the image via TensorFlow.io.read_file\n",
    "    imageBinary = TensorFlow.io.read_file(absolutePath)\n",
    "    # Decode the binary via TensorFlow.io.decode_jpeg\n",
    "    imageRaw = TensorFlow.io.decode_jpeg(imageBinary, channels=3)\n",
    "    # Resize the raw image in case that the image is not 64x64 for some reason\n",
    "    imageRawResized = TensorFlow.image.resize(\n",
    "        imageRaw, IMAGE_SIZE, method=TensorFlow.image.ResizeMethod.BICUBIC\n",
    "    )\n",
    "    return imageRawResized\n",
    "\n",
    "def datasetMapper(stringTensor:TensorFlow.Tensor) -> tuple:\n",
    "    # Start with the first image\n",
    "    # First, trim the path to just the filename + extension\n",
    "    pathParts = TensorFlow.strings.split(stringTensor[0], \"/\")\n",
    "    fileName = pathParts[-1]\n",
    "    # We need to have an absolute path, not a relative one\n",
    "    absolutePath = TensorFlow.strings.join([DIR_PATH, os.path.sep, fileName])\n",
    "    # Then, load the image\n",
    "    image1 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Now do it again for image 2\n",
    "    pathParts = TensorFlow.strings.split(stringTensor[1], \"/\")\n",
    "    fileName = pathParts[-1]\n",
    "    absolutePath = TensorFlow.strings.join([DIR_PATH, os.path.sep, fileName])\n",
    "    image2 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Lastly, the label should be a float, this can be done by checking if\n",
    "    # the tensor value is zero or not\n",
    "    parity = TensorFlow.cond(\n",
    "        stringTensor[2] == stringZeroTensor,\n",
    "        lambda: floatZeroTensor,\n",
    "        lambda: floatOneTensor\n",
    "    )\n",
    "    \n",
    "    # Images and label are now good, return them\n",
    "    return image1, image2, parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39e008",
   "metadata": {},
   "source": [
    "Finally, map the dataset to a new one. Let TensorFlow decide how to do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d47e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn stringTensor into a dataset first\n",
    "stringDataset = TensorFlow.data.Dataset.from_tensor_slices(stringTensor)\n",
    "# Now map the new dataset\n",
    "dataset = stringDataset.map(\n",
    "    datasetMapper, num_parallel_calls=TensorFlow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Test sample\n",
    "for image1, image2, label in dataset.take(1):\n",
    "    print(\"Shape 1 =\", image1.numpy().shape)\n",
    "    print(\"Shape 2 =\", image2.numpy().shape)\n",
    "    print(\"Label =\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f4ccf",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Modification and Optimization\n",
    "There is no notion of a training set nor testing set in the dataset yet, so we have to create those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f295c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset first\n",
    "shuffledDataset = dataset.shuffle(TRAINING_PAIRS + TESTING_PAIRS, reshuffle_each_iteration=False)\n",
    "# Now make the training and testing datasets\n",
    "trainingDataset = shuffledDataset.take(TRAINING_PAIRS)\n",
    "testingDataset = shuffledDataset.skip(TRAINING_PAIRS)\n",
    "print(trainingDataset.cardinality())\n",
    "print(testingDataset.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454652df",
   "metadata": {},
   "source": [
    "Okay, now the datasets need to be optimized to make sure disk I/O does not slow down training unecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeDataset(dataset:TensorFlow.data.Dataset) -> TensorFlow.data.Dataset:\n",
    "    return dataset.cache().batch(PAIRS_PER_BATCH).prefetch(buffer_size=TensorFlow.data.AUTOTUNE)\n",
    "\n",
    "trainingOptimizedDataset = optimizeDataset(trainingDataset)\n",
    "testingOptimizedDataset = optimizeDataset(testingDataset)\n",
    "\n",
    "# Test sample\n",
    "print(trainingOptimizedDataset.cardinality())\n",
    "print(testingOptimizedDataset.cardinality())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
