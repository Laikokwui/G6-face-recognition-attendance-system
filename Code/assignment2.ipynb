{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711e13bd",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "**All codes in Section 0 need to be run before any models can be built.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b780870",
   "metadata": {},
   "source": [
    "## 0.1 Constants Declaration\n",
    "**Constants that represent absolute paths should be changed to match the folder and file locations of the inputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6df75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\classification\\train\"\n",
    "TESTING_DIR = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\classification\\test\"\n",
    "TESTING_PAIRS_TXT = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_pairs_val.txt\"\n",
    "TESTING_PAIRS_DIR = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\"\n",
    "IMAGE_SIZE = (64,64)\n",
    "BATCH_SIZE = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "#DIR_PATH = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_data\"\n",
    "#TXT_PATH = r\"D:\\ARCHIVED\\University Files\\VI\\aml\\a2\\verification_pairs_val.txt\"\n",
    "#TRAINING_PAIRS = 4096\n",
    "#TESTING_PAIRS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead04dd0",
   "metadata": {},
   "source": [
    "## 0.2 Simple ML Builder Class\n",
    "This class is created entirely by hand, to simplify the process of building and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0007d388",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Loading compulsory modules...\n",
      "[SMLB] Loading TensorFlow... this will take a while.\n",
      "[SMLB] TensorFlow loaded! TensorFlow version is 2.7.0.\n",
      "[SMLB] Loading NumPy...\n",
      "[SMLB] NumPy loaded!\n",
      "[SMLB] Loading PyPlot...\n",
      "[SMLB] PyPlot loaded!\n",
      "[SMLB] All compulsory imports successful!\n",
      "[SMLB] Loading optional modules...\n",
      "[SMLB] Loading TensorFlow Addons...\n",
      "[SMLB] TensorFlow Addons loaded! TensorFlow Addons version is 0.15.0.\n",
      "[SMLB] Loading SciKitLearn...\n",
      "[SMLB] SciKitLearn loaded!\n",
      "[SMLB] Optional module imports processed.\n",
      "[SMLB] Initialization successful!\n"
     ]
    }
   ],
   "source": [
    "import sys, math\n",
    "\n",
    "def smlb_log(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep)\n",
    "\n",
    "def smlb_log_error(*message, sep:str=\" \"):\n",
    "\tprint(\"[SMLB]\", *message, sep=sep, file=sys.stderr)\n",
    "\n",
    "smlb_log(\"Loading compulsory modules...\")\n",
    "\n",
    "smlb_log(\"Loading TensorFlow... this will take a while.\")\n",
    "import tensorflow as TensorFlow\n",
    "Keras = TensorFlow.keras\n",
    "smlb_log(\"TensorFlow loaded! TensorFlow version is\", TensorFlow.__version__ + \".\")\n",
    "\n",
    "smlb_log(\"Loading NumPy...\")\n",
    "import numpy as NumPy\n",
    "smlb_log(\"NumPy loaded!\")\n",
    "\n",
    "smlb_log(\"Loading PyPlot...\")\n",
    "from matplotlib import pyplot as PyPlot\n",
    "smlb_log(\"PyPlot loaded!\")\n",
    "\n",
    "smlb_log(\"All compulsory imports successful!\")\n",
    "smlb_log(\"Loading optional modules...\")\n",
    "\n",
    "smlb_log(\"Loading TensorFlow Addons...\")\n",
    "try:\n",
    "\timport tensorflow_addons as TensorFlowAddons\n",
    "except ImportError:\n",
    "\tsmlb_log(\"TensorFlow Addons not installed.\")\n",
    "else:\n",
    "\tsmlb_log(\"TensorFlow Addons loaded! TensorFlow Addons version is\", TensorFlowAddons.__version__ + \".\")\n",
    "\n",
    "smlb_log(\"Loading SciKitLearn...\")\n",
    "try:\n",
    "\timport sklearn\n",
    "except ImportError:\n",
    "\tsmlb_log(\"SciKitLearn not installed.\")\n",
    "else:\n",
    "\tsmlb_log(\"SciKitLearn loaded!\")\n",
    "\t\n",
    "smlb_log(\"Optional module imports processed.\")\n",
    "\n",
    "class SimpleMLBuilder:\n",
    "\tdef __init__(self, verbose:bool=False):\n",
    "\t\tself.datasets = {\"training\": [None, None], \"validation\": None, \"testing\": [None, None]}\n",
    "\t\tself.layers = []\n",
    "\t\tself.labels = []\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.history = None\n",
    "\t\tself.log(\"Fully initialized!\")\n",
    "\t\n",
    "\tdef log(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log(*message, sep=sep)\n",
    "\t\n",
    "\tdef log_error(self, *message, sep:str=\" \", nonVerbose:bool=False):\n",
    "\t\tif self.verbose or nonVerbose:\n",
    "\t\t\tsmlb_log_error(*message, sep=sep)\n",
    "\t\n",
    "\tdef load_preset_dataset(self, preset:str):\n",
    "\t\t\"\"\"Loads a preset dataset with Keras.\n",
    "\t\t\n",
    "\t\tBuilt-in presets: MNIST & Fashion MNIST.\n",
    "\t\tUseful for testing the SMLB.\n",
    "\t\t\"\"\"\n",
    "\t\tpreset = preset.lower()\n",
    "\t\tif preset == \"mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.mnist.load_data()\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset.\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"MNIST\\\" loaded successfully.\")\n",
    "\t\telif preset == \"fashion mnist\":\n",
    "\t\t\tself.log(\"Loading preset \\\"Fashion MNIST\\\"...\")\n",
    "\t\t\t\n",
    "\t\t\t(trainingXs, trainingYs), (testingXs, testingYs) = Keras.datasets.fashion_mnist.load_data()\n",
    "\t\t\tself.labels = [\n",
    "\t\t\t\tNone,\n",
    "\t\t\t\t[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\t\t\t\t\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\t\t\t]\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"training\"] = [trainingXs, trainingYs]\n",
    "\t\t\tself.log(\"Training set loaded from preset. Training images has shape of\", str(trainingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.datasets[\"testing\"] = [testingXs, testingYs]\n",
    "\t\t\tself.log(\"Testing set loaded from preset. Testing images has shape of\", str(testingXs.shape) + \".\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Preset \\\"Fashion MNIST\\\" loaded successfully.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Preset \\\"\" + preset + \"\\\" not found.\")\n",
    "\t\n",
    "\tdef set_training_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for training.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][0] = features\n",
    "\t\tself.log(\"Training X values have been set!\")\n",
    "\tdef set_training_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for training.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the training features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"training\"][1] = labels\n",
    "\t\tself.log(\"Training Y values have been set!\")\n",
    "\tdef set_testing_features(self, features):\n",
    "\t\t\"\"\"Sets the features used for testing.\n",
    "\t\t\n",
    "\t\tFeatures can be a list of entries or a dataset.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][0] = features\n",
    "\t\tself.log(\"Testing X values have been set!\")\n",
    "\tdef set_testing_labels(self, labels=None):\n",
    "\t\t\"\"\"Sets the labels used for testing.\n",
    "\t\t\n",
    "\t\tLabels should be a list of entries.\n",
    "\t\tIf a TensorFlow.data.Dataset is used for the testing features, the labels specified here are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"testing\"][1] = labels\n",
    "\t\tself.log(\"Testing Y values have been set!\")\n",
    "\tdef set_validation_dataset(self, dataset):\n",
    "\t\t\"\"\"Sets the dataset used for validation.\n",
    "\t\t\n",
    "\t\tDataset should either be a TensorFlow.data.Dataset or a list of feature-label tuples.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.datasets[\"validation\"] = dataset\n",
    "\t\tself.log(\"Validation dataset has been set!\")\n",
    "\t\n",
    "\tdef get_training_features(self):\n",
    "\t\treturn self.datasets[\"training\"][0]\n",
    "\tdef get_training_labels(self):\n",
    "\t\treturn self.datasets[\"training\"][1]\n",
    "\tdef get_testing_features(self):\n",
    "\t\treturn self.datasets[\"testing\"][0]\n",
    "\tdef get_testing_labels(self):\n",
    "\t\treturn self.datasets[\"testing\"][1]\n",
    "\t\n",
    "\tdef get_feature_classes(self):\n",
    "\t\treturn self.labels[0]\n",
    "\tdef get_label_classes(self, y:bool=False):\n",
    "\t\treturn self.labels[1]\n",
    "\t\n",
    "\tdef start_layering(self, inputShape:tuple=None):\n",
    "\t\t\"\"\"Starts the creation of a new model.\n",
    "\t\t\n",
    "\t\tThis method also creates an input layer and adds it to the model.\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.layers.clear()\n",
    "\t\tlayer = {\"type\": \"Input\", \"shape\": inputShape}\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Layering started. Added layer:\", layer)\n",
    "\t\n",
    "\tdef _add_layer(self, layer:dict):\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Added layer:\", layer)\n",
    "\tdef add_dense_layer(self, neurons:int):\n",
    "\t\t\"\"\"Adds a densely-connected layer to the model.\n",
    "\t\t\n",
    "\t\tDense layers are the bread and butter of any Deep Neural Network.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dense\", \"units\": neurons}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_activation_layer(self, activation:str=None):\n",
    "\t\t\"\"\"Adds an activation layer to the model.\n",
    "\t\t\n",
    "\t\tPossible activations: relu.\n",
    "\t\tIf there is a dense or convolution layer before this layer, that layer will be modified instead.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tlayerType = layer[\"type\"]\n",
    "\t\tif layerType == \"Dense\" or layerType == \"Conv2D\":\n",
    "\t\t\tlayer[\"activation\"] = activation\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself.log(\"Modified layer:\", layer)\n",
    "\t\telse:\n",
    "\t\t\tself.layers.append(layer)\n",
    "\t\t\tself._add_layer({\"type\": activation})\n",
    "\tdef add_rescaling_layer(self, scale:float=1.0/255, offset:float=0.0):\n",
    "\t\t\"\"\"Adds a rescaling layer to the model.\n",
    "\t\t\n",
    "\t\tUsed to add and multiply values of the previous layer.\n",
    "\t\tTypically used as a preprocessing layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Rescaling\", \"scale\": scale, \"offset\": offset}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_regularization_sublayer(self, regularization:str=None, regAmount:float=0.0):\n",
    "\t\t\"\"\"Modifies the previous layer to use regularization.\n",
    "\t\t\n",
    "\t\tPossible regularizations: l1, l2.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = self.layers.pop()\n",
    "\t\tif regularization==\"l1\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l1(regAmount)\n",
    "\t\telif regularization==\"l2\":\n",
    "\t\t\tlayer[\"kernel_regularizer\"] = Keras.regularizers.l2(regAmount)\n",
    "\t\tself.layers.append(layer)\n",
    "\t\tself.log(\"Modified layer:\", layer)\n",
    "\tdef add_regularization_layer(self, regularization:str=None, regAmount:float=0.0):\n",
    "\t\t\"\"\"Adds a regularization layer to the model, as a lambda layer.\n",
    "\t\t\n",
    "\t\tIf this layer is being attached to the previous layer, consider using add_regularization_sublayer instead.\n",
    "\t\tPossible regularizations: l2.\n",
    "\t\t\"\"\"\n",
    "\t\tif regularization==\"l2\":\n",
    "\t\t\tlayer = {\"type\": \"Lambda\", \"function\": lambda x: TensorFlow.math.l2_normalize(x, axis=1)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_flatten_layer(self):\n",
    "\t\t\"\"\"Adds a flattening layer to the model.\n",
    "\t\t\n",
    "\t\tFlattening layers turn a n-dimensional input into a (n-1)-dimensional input,\n",
    "\t\twhere each vector in the tensor is concatenated with the last.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Flatten\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_softmax_layer(self):\n",
    "\t\t\"\"\"Adds a softmax layer to the model.\n",
    "\t\t\n",
    "\t\tSoftmax layers turn a tensor of logistic values into a tensor of probablistic values.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Softmax\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_dropout_layer(self, probability:float):\n",
    "\t\t\"\"\"Adds a dropout layer to the model.\n",
    "\t\t\n",
    "\t\tDropout layers have a chance to output 0 instead of the previous layer's values.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Dropout\", \"rate\": probability}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_normalization_layer(self, axis:int=None):\n",
    "\t\t\"\"\"Normalizes input to be within a normal distribution of mean 0 and standard variance 1.\"\"\"\n",
    "\t\tlayer = {\"type\": \"Normalization\", \"axis\": axis}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_convolution_layer(self, filters:int, filterSize:tuple, stride:tuple=(1,1), pad:bool=False):\n",
    "\t\t\"\"\"Adds a convolution layer to the model.\n",
    "\t\t\n",
    "\t\tConvolution layers help to get certain image data features of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tlayer = {\"type\": \"Conv2D\", \"filters\": filters, \"kernel_size\": filterSize, \"strides\": stride, \"padding\": \"same\" if pad else \"valid\"}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_pooling_layer(self, method:str=\"max\", filterSize:tuple=(1,1), stride:tuple=(1,1)):\n",
    "\t\t\"\"\"Adds a pooling layer to the model.\n",
    "\t\t\n",
    "\t\tPooling layers help to summarize image data of the previous layer.\n",
    "\t\t\"\"\"\n",
    "\t\tif method==\"max\":\n",
    "\t\t\tlayer = {\"type\": \"MaxPool2D\", \"pool_size\": filterSize, \"strides\": stride}\n",
    "\t\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef add_random_contrast_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Adds random image contrast to the input and outputs it.\n",
    "\t\t\n",
    "\t\tInput can be negative to reduce image contrast.\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\telse:\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomContrast\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_flip_layer(self, x:bool=False, y:bool=False):\n",
    "\t\t\"\"\"Has a 50% chance to flip the input around a given axis.\n",
    "\t\t\n",
    "\t\tx = allow horizontal flip, y = allow vertical flip\n",
    "\t\t\"\"\"\n",
    "\t\tflip = y and [x and \"horizontal_and_vertical\" or \"vertical\"] or \"horizontal\"\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomFlip\", \"mode\": flip}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_rotation_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly rotates the input around its center clockwise by the given amount of radians.\n",
    "\t\t\n",
    "\t\tInput can be negative to rotate counter-clockwise.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomRotation\", \"factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\tdef add_random_zoom_layer(self, minimum:float, maximum:float=None):\n",
    "\t\t\"\"\"Randomly zooms the input image by the given multiplier.\n",
    "\t\t\n",
    "\t\tInput can be negative to zoom out, up to > -1.\n",
    "\t\t\"\"\"\n",
    "\t\tif not maximum:\n",
    "\t\t\tmaximum = minimum\n",
    "\t\t\tminimum = -minimum\n",
    "\t\t\n",
    "\t\tlayer = {\"type\": \"RandomZoom\", \"height_factor\": (minimum, maximum)}\n",
    "\t\tself._add_layer(layer)\n",
    "\t\n",
    "\tdef get_layers(self) -> list:\n",
    "\t\treturn self.layers\n",
    "\t\n",
    "\tdef set_scc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.SparseCategoricalCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_bc_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to TensorFlow.keras.losses.BinaryCrossentropy.\n",
    "\t\t\n",
    "\t\tAlways softmaxes input.\n",
    "\t\t\"\"\"\n",
    "\t\tself.lossFunction = Keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_mae_loss_function(self):\n",
    "\t\t\"\"\"Sets the loss function to the mean absolute error.\"\"\"\n",
    "\t\tself.lossFunction = \"mean_absolute_error\"\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\tdef set_custom_loss_function(self, loss:Keras.losses):\n",
    "\t\t\"\"\"Sets the loss function to the passed value.\"\"\"\n",
    "\t\tself.lossFunction = loss\n",
    "\t\tself.log(\"Set loss function:\", self.lossFunction)\n",
    "\t\n",
    "\tdef _create_layer(self, layerData:dict) -> Keras.layers.Layer:\n",
    "\t\tlayerType = layerData.pop(\"type\")\n",
    "\t\tlayer = None\n",
    "\t\tif layerType == \"Input\":\n",
    "\t\t\tlayer = Keras.Input(**layerData)\n",
    "\t\telse:\n",
    "\t\t\tlayerCreationFunc = getattr(Keras.layers, layerType)\n",
    "\t\t\tlayer = layerCreationFunc(**layerData)\n",
    "\t\t\tif layerType == \"Normalization\":\n",
    "\t\t\t\tdataset = self.datasets[\"training\"][0]\n",
    "\t\t\t\tif dataset is TensorFlow.data.Dataset:\n",
    "\t\t\t\t\tlayer.adapt(dataset)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlayer.adapt(NumPy.array(dataset))\n",
    "\t\treturn layer\n",
    "\tdef build(self, learningRate:float=0.001, additionalMetrics:list=[]):\n",
    "\t\t\"\"\"Builds and compiles the model based on the layers added.\"\"\"\n",
    "\t\tif len(self.layers)==0:\n",
    "\t\t\tself.log_error(\"Please add layers to the builder template before building.\")\n",
    "\t\telif not hasattr(self, \"lossFunction\"):\n",
    "\t\t\tself.log_error(\"Please specify the loss function first.\")\n",
    "\t\telse:\n",
    "\t\t\tself.log(\" Building model with learning rate = \", learningRate, \"...\", sep=\"\")\n",
    "\t\t\t\n",
    "\t\t\tkerasLayers = []\n",
    "\t\t\tfor layer in self.layers:\n",
    "\t\t\t\tkerasLayers.append(self._create_layer(layer))\n",
    "\t\t\tmodel = Keras.models.Sequential(kerasLayers)\n",
    "\t\t\tif self.lossFunction is not Keras.losses.SparseCategoricalCrossentropy:\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction,\n",
    "\t\t\t\t\tmetrics=additionalMetrics\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.compile(\n",
    "\t\t\t\t\toptimizer=TensorFlow.optimizers.Adam(learning_rate=learningRate),\n",
    "\t\t\t\t\tloss=self.lossFunction,\n",
    "\t\t\t\t\tmetrics=[\"accuracy\"] + additionalMetrics\n",
    "\t\t\t\t)\n",
    "\t\t\tself.compiledModel = model\n",
    "\t\t\tself.log(\"Model built! Details:\")\n",
    "\t\t\tmodel.summary()\n",
    "\tdef build_and_eject_before_compilation(self) -> Keras.Model:\n",
    "\t\t\"\"\"Builds the model based on the layers added, but ejects the model before compilation.\"\"\"\n",
    "\t\tif len(self.layers)==0:\n",
    "\t\t\tself.log_error(\"Please add layers to the builder template before building.\")\n",
    "\t\telse:\n",
    "\t\t\tkerasLayers = []\n",
    "\t\t\tfor layer in self.layers:\n",
    "\t\t\t\tkerasLayers.append(self._create_layer(layer))\n",
    "\t\t\treturn Keras.models.Sequential(kerasLayers)\n",
    "\t\n",
    "\tdef destroy(self):\n",
    "\t\tdel self.compiledModel\n",
    "\t\tself.log(\"Model destroyed!\")\n",
    "\t\n",
    "\tdef set_compiled_model(self, model:Keras.Model):\n",
    "\t\tself.compiledModel = model\n",
    "\tdef get_compiled_model(self) -> Keras.Model:\n",
    "\t\treturn self.compiledModel\n",
    "\t\n",
    "\tdef save(self, name:str=\"Unnamed\"):\n",
    "\t\tif hasattr(self, \"compiledModel\"):\n",
    "\t\t\tself.log(\"Saving model...\")\n",
    "\t\t\tself.compiledModel.save(name)\n",
    "\t\t\tself.log(\"Save complete!\")\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"No model to save!\")\n",
    "\tdef load(self, name:str):\n",
    "\t\tself.log(\"Loading model...\")\n",
    "\t\tself.compiledModel = Keras.models.load_model(name)\n",
    "\t\tself.log(\"Load complete!\")\n",
    "\t\n",
    "\tdef get_history(self):\n",
    "\t\treturn self.history\n",
    "\t\n",
    "\tdef _create_early_stopping_callback(self, epochs:int, hasValidation:bool=False) -> Keras.callbacks.EarlyStopping:\n",
    "\t\treturn Keras.callbacks.EarlyStopping(monitor=\"val_loss\" if hasValidation else \"loss\", mode=\"min\", patience=math.ceil(epochs ** 0.5), restore_best_weights=True)\n",
    "\t\n",
    "\tdef run(self, epochs:int, validationSplit:float=0.0, earlyStop:bool=False):\t\n",
    "\t\t\"\"\"Causes the model to start trying to fit to the training data.\n",
    "\t\t\n",
    "\t\tvalidationSplit is ignored if the validation dataset was specified via set_validation_dataset().\n",
    "\t\tearlyStop causes the model to stop training if the validation loss (or the training loss if no validation specified) does not improve after the square root amount of epochs, rounded up.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tfitArguments = {\n",
    "\t\t\t\t\"x\": self.datasets[\"training\"][0],\n",
    "\t\t\t\t\"y\": self.datasets[\"training\"][1],\n",
    "\t\t\t\t\"epochs\": epochs,\n",
    "\t\t\t\t\"validation_data\": self.datasets[\"validation\"],\n",
    "\t\t\t\t\"validation_split\": validationSplit\n",
    "\t\t\t}\n",
    "\t\t\tif earlyStop:\n",
    "\t\t\t\tfitArguments[\"callbacks\"] = [self._create_early_stopping_callback(epochs, True if self.datasets[\"validation\"] else validation_split > 0.0)]\n",
    "\t\t\tself.history = self.compiledModel.fit(**fitArguments)\n",
    "\t\t\tself.log(\"======== TRAINING DONE ========\", nonVerbose=True)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef plot(self):\n",
    "\t\t\"\"\"Plots the training progress via MatPlotLib.\"\"\"\n",
    "\t\tif self.history:\n",
    "\t\t\tself.log(\"Creating graphs, please wait...\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\thistoryDict = self.history.history\n",
    "\t\t\t\n",
    "\t\t\tepochsRange = range(self.history.params[\"epochs\"])\n",
    "\t\t\taccuracy = historyDict[\"accuracy\"]\n",
    "\t\t\tloss = historyDict[\"loss\"]\n",
    "\t\t\t\n",
    "\t\t\tif \"val_accuracy\" in historyDict:\n",
    "\t\t\t\tvalidationAccuracy = historyDict[\"val_accuracy\"]\n",
    "\t\t\t\tvalidationLoss = historyDict[\"val_loss\"]\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.figure(figsize=(12, 6))\n",
    "\t\t\tPyPlot.subplot(1, 2, 1)\n",
    "\t\t\tPyPlot.plot(epochsRange, accuracy, label=\"Training Accuracy\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationAccuracy, label=\"Validation Accuracy\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Accuracy\")\n",
    "\t\t\t\n",
    "\t\t\tPyPlot.subplot(1, 2, 2)\n",
    "\t\t\tPyPlot.plot(epochsRange, loss, label=\"Training Loss\")\n",
    "\t\t\tPyPlot.plot(epochsRange, validationLoss, label=\"Validation Loss\")\n",
    "\t\t\tPyPlot.legend()\n",
    "\t\t\tPyPlot.title(\"Loss\")\n",
    "\t\t\tPyPlot.show()\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please run the model first.\")\n",
    "\t\n",
    "\tdef evaluate(self) -> tuple:\n",
    "\t\t\"\"\"Evaluates the model over the given training dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tresults = self.compiledModel.evaluate(self.datasets[\"testing\"][0], self.datasets[\"testing\"][1], verbose=2)\n",
    "\t\t\tself.log(\"======== TESTING DONE ========\", nonVerbose=True)\n",
    "\t\t\t\n",
    "\t\t\treturn results\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build the model first.\")\n",
    "\t\n",
    "\tdef predict(self, features) -> NumPy.ndarray:\n",
    "\t\t\"\"\"Makes the model do predictions over the given testing dataset.\"\"\"\n",
    "\t\tif self.compiledModel:\n",
    "\t\t\tpredictionModel = Keras.Sequential([self.compiledModel, Keras.layers.Softmax()])\n",
    "\t\t\treturn predictionModel.predict(features)\n",
    "\t\telse:\n",
    "\t\t\tself.log_error(\"Please build and train the model first.\")\n",
    "\n",
    "smlb_log(\"Initialization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673482f7",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation\n",
    "## 1.1 Training and Validation Datasets\n",
    "### 1.1.1 Creation\n",
    "We can use `tf.keras.utils.image_dataset_from_directory` to create the `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae2e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 200 classes.\n",
      "Using 4500 files for training.\n",
      "Found 5000 files belonging to 200 classes.\n",
      "Using 500 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed = random.randrange(0, 16777216)\n",
    "trainingDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"training\", interpolation=\"bicubic\"\n",
    ")\n",
    "validationDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"validation\", interpolation=\"bicubic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a68bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(45, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Test sample\n",
    "print(trainingDataset.cardinality())\n",
    "print(validationDataset.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6267",
   "metadata": {},
   "source": [
    "### 1.1.2 Optimization\n",
    "The datasets need to be optimized to make sure disk I/O does not slow down training unecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b33c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingOptimizedDataset = trainingDataset.cache().shuffle(1000).prefetch(buffer_size=TensorFlow.data.AUTOTUNE)\n",
    "validationOptimizedDataset = validationDataset.cache().prefetch(buffer_size=TensorFlow.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7bf98d",
   "metadata": {},
   "source": [
    "## 1.2 Testing Dataset\n",
    "The same steps are done for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1b07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 files belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "testingDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TESTING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed, interpolation=\"bicubic\"\n",
    ")\n",
    "testingOptimizedDataset = testingDataset.cache().prefetch(buffer_size=TensorFlow.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c33ad",
   "metadata": {},
   "source": [
    "The final datasets, `trainingOptimizedDataset`, `validationOptimizedDataset` and `testingOptimizedDataset`, can now be used for training the model.\n",
    "# 2. Model Building\n",
    "The models that need to be built are a cascading series of layers with triplet loss.\n",
    "## 2.1. Basic Model\n",
    "The following is the first architecture used:\n",
    "- Input Rescaler (factor=1/255)\n",
    "- Padded 3x3 Convolution Stride 1x1 (x64) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x64) with ReLU activation\n",
    "- 2x2 Max Pooling Stride 2x2\n",
    "- Padded 3x3 Convolution Stride 1x1 (x128) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x128) with ReLU activation\n",
    "- 2x2 Max Pooling Stride 2x2\n",
    "- Padded 3x3 Convolution Stride 1x1 (x256) with ReLU activation\n",
    "- Padded 3x3 Convolution Stride 1x1 (x256) with ReLU activation\n",
    "- Axis Flattener\n",
    "- Fully Connected (x256)\n",
    "- Softmax as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb88f6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Fully initialized!\n",
      "[SMLB] Training X values have been set!\n",
      "[SMLB] Validation dataset has been set!\n",
      "[SMLB] Testing X values have been set!\n",
      "[SMLB] Layering started. Added layer: {'type': 'Input', 'shape': (64, 64, 3)}\n",
      "[SMLB] Added layer: {'type': 'Rescaling', 'scale': 0.00392156862745098, 'offset': 0.0}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Flatten'}\n",
      "[SMLB] Added layer: {'type': 'Dense', 'units': 256}\n",
      "[SMLB] Modified layer: {'type': 'Dense', 'units': 256, 'kernel_regularizer': <keras.regularizers.L2 object at 0x000001C23F714640>}\n",
      "[SMLB] Set loss function: <tensorflow_addons.losses.triplet.TripletSemiHardLoss object at 0x000001C23F714880>\n",
      "[SMLB] Building model with learning rate = 0.001...\n",
      "[SMLB] Model built! Details:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 65536)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16777472  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,922,880\n",
      "Trainable params: 17,922,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smlb = SimpleMLBuilder(True)\n",
    "smlb.set_training_features(trainingOptimizedDataset)\n",
    "smlb.set_validation_dataset(validationOptimizedDataset)\n",
    "smlb.set_testing_features(testingOptimizedDataset)\n",
    "\n",
    "smlb.start_layering((64, 64, 3))\n",
    "smlb.add_rescaling_layer()\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_flatten_layer()\n",
    "smlb.add_dense_layer(256)\n",
    "smlb.add_regularization_sublayer(\"l2\")\n",
    "\n",
    "smlb.set_custom_loss_function(TensorFlowAddons.losses.TripletSemiHardLoss())\n",
    "smlb.build(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81b32fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 54s 236ms/step - loss: 1.3356 - val_loss: 0.1165\n",
      "[SMLB] ======== TRAINING DONE ========\n"
     ]
    }
   ],
   "source": [
    "smlb.run(epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca6669",
   "metadata": {},
   "source": [
    "## 2.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0602295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 4s - loss: 0.1319 - 4s/epoch - 1s/step\n",
      "[SMLB] ======== TESTING DONE ========\n",
      "[SMLB] Saving model...\n",
      "INFO:tensorflow:Assets written to: Model 1\\assets\n",
      "[SMLB] Save complete!\n"
     ]
    }
   ],
   "source": [
    "smlb.evaluate()\n",
    "smlb.save(\"Model 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8b1e9",
   "metadata": {},
   "source": [
    "## 2.3 Testing Dataset\n",
    "The problem with the model is that it cannot produce an ROC curve on its own (since it just produces face embeddings). To solve this, we need to gather all of the image file paths and parity labels as indicated by `FACE_VERIFICATION_TXT_PATH`, and calculate the distance of both images in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314ec7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "stringList = []\n",
    "with open(TESTING_PAIRS_TXT) as fileHandle:\n",
    "    for line in fileHandle:\n",
    "        stringList.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c464f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8805 verification_data/00041961.jpg\n"
     ]
    }
   ],
   "source": [
    "print(len(stringList), stringList[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c571e6b",
   "metadata": {},
   "source": [
    "After reading from the file, we need to turn `stringList` into a `tf.data.Dataset`. This is not trivial.\n",
    "\n",
    "The first step is to turn the above list into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbefb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringTensor = TensorFlow.constant(stringList, TensorFlow.dtypes.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17fd5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'verification_data/00041961.jpg' b'verification_data/00044353.jpg' b'0'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(stringTensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc46f1",
   "metadata": {},
   "source": [
    "Now we need a function that returns the image data as a tensor of floats and the label as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cdec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stringZeroTensor = TensorFlow.constant('0', TensorFlow.dtypes.string)\n",
    "floatZeroTensor = TensorFlow.constant(0, TensorFlow.dtypes.float32)\n",
    "floatOneTensor = TensorFlow.constant(1, TensorFlow.dtypes.float32)\n",
    "\n",
    "def getImageTensorByAbsolutePath(absolutePath:TensorFlow.Tensor)\\\n",
    "-> TensorFlow.Tensor:\n",
    "    # Load the image via TensorFlow.io.read_file\n",
    "    imageBinary = TensorFlow.io.read_file(absolutePath)\n",
    "    # Decode the binary via TensorFlow.io.decode_jpeg\n",
    "    imageRaw = TensorFlow.io.decode_jpeg(imageBinary, channels=3)\n",
    "    # Resize the raw image in case that the image is not 64x64 for some reason\n",
    "    imageRawResized = TensorFlow.image.resize(\n",
    "        imageRaw, IMAGE_SIZE, method=TensorFlow.image.ResizeMethod.BICUBIC\n",
    "    )\n",
    "    return imageRawResized\n",
    "\n",
    "def datasetMapper(stringTensor:TensorFlow.Tensor) -> tuple:\n",
    "    # Start with the first image\n",
    "    # First, get the relative path\n",
    "    relativePath = stringTensor[0]\n",
    "    # We need to have an absolute path, not a relative one, so convert it\n",
    "    absolutePath = TensorFlow.strings.join([TESTING_PAIRS_DIR, os.path.sep, relativePath])\n",
    "    # Then, load the image\n",
    "    image1 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Now do it again for image 2\n",
    "    relativePath = stringTensor[1]\n",
    "    absolutePath = TensorFlow.strings.join([TESTING_PAIRS_DIR, os.path.sep, relativePath])\n",
    "    image2 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Lastly, the label should be a float, this can be done by checking if\n",
    "    # the tensor value is zero or not\n",
    "    parity = TensorFlow.cond(\n",
    "        stringTensor[2] == stringZeroTensor,\n",
    "        lambda: floatZeroTensor,\n",
    "        lambda: floatOneTensor\n",
    "    )\n",
    "    \n",
    "    # Images and label are now good, return them\n",
    "    return image1, image2, parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39e008",
   "metadata": {},
   "source": [
    "Finally, map the dataset to a new one. Let TensorFlow decide how to do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64d47e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 = tf.Tensor(\n",
      "[[[188. 192. 195.]\n",
      "  [187. 191. 194.]\n",
      "  [187. 191. 194.]\n",
      "  ...\n",
      "  [199. 200. 204.]\n",
      "  [200. 201. 205.]\n",
      "  [201. 202. 206.]]\n",
      "\n",
      " [[187. 191. 194.]\n",
      "  [187. 191. 194.]\n",
      "  [186. 190. 193.]\n",
      "  ...\n",
      "  [198. 199. 203.]\n",
      "  [198. 199. 203.]\n",
      "  [199. 200. 204.]]\n",
      "\n",
      " [[187. 191. 194.]\n",
      "  [187. 191. 194.]\n",
      "  [186. 190. 193.]\n",
      "  ...\n",
      "  [197. 198. 200.]\n",
      "  [197. 198. 200.]\n",
      "  [198. 199. 201.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 34.  37.  46.]\n",
      "  [ 34.  37.  46.]\n",
      "  [ 33.  36.  45.]\n",
      "  ...\n",
      "  [177. 101. 111.]\n",
      "  [191.  79.  91.]\n",
      "  [201.  70.  84.]]\n",
      "\n",
      " [[ 33.  36.  45.]\n",
      "  [ 32.  35.  44.]\n",
      "  [ 32.  35.  44.]\n",
      "  ...\n",
      "  [192. 127. 133.]\n",
      "  [190.  80.  91.]\n",
      "  [206.  74.  87.]]\n",
      "\n",
      " [[ 31.  34.  43.]\n",
      "  [ 31.  34.  43.]\n",
      "  [ 30.  33.  42.]\n",
      "  ...\n",
      "  [207. 146. 151.]\n",
      "  [189.  79.  90.]\n",
      "  [209.  74.  88.]]], shape=(64, 64, 3), dtype=float32)\n",
      "Image 2 = tf.Tensor(\n",
      "[[[166. 169. 186.]\n",
      "  [164. 167. 182.]\n",
      "  [152. 155. 170.]\n",
      "  ...\n",
      "  [196. 197. 192.]\n",
      "  [207. 206. 202.]\n",
      "  [201. 200. 196.]]\n",
      "\n",
      " [[160. 163. 180.]\n",
      "  [161. 164. 179.]\n",
      "  [158. 162. 174.]\n",
      "  ...\n",
      "  [191. 193. 190.]\n",
      "  [197. 197. 195.]\n",
      "  [194. 194. 192.]]\n",
      "\n",
      " [[164. 167. 182.]\n",
      "  [157. 160. 175.]\n",
      "  [152. 156. 168.]\n",
      "  ...\n",
      "  [194. 198. 197.]\n",
      "  [193. 199. 197.]\n",
      "  [183. 189. 187.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 32.  41.  58.]\n",
      "  [ 32.  40.  59.]\n",
      "  [ 38.  46.  69.]\n",
      "  ...\n",
      "  [  8.   9.  11.]\n",
      "  [ 13.  13.  13.]\n",
      "  [ 12.  12.  10.]]\n",
      "\n",
      " [[ 33.  44.  62.]\n",
      "  [ 35.  46.  66.]\n",
      "  [ 33.  39.  65.]\n",
      "  ...\n",
      "  [  8.   9.  13.]\n",
      "  [  8.   8.  10.]\n",
      "  [  8.   8.   8.]]\n",
      "\n",
      " [[ 24.  37.  54.]\n",
      "  [ 30.  41.  61.]\n",
      "  [ 29.  33.  60.]\n",
      "  ...\n",
      "  [  9.  10.  15.]\n",
      "  [ 11.  10.  15.]\n",
      "  [  9.   9.  11.]]], shape=(64, 64, 3), dtype=float32)\n",
      "Label = tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Turn stringTensor into a dataset first\n",
    "stringDataset = TensorFlow.data.Dataset.from_tensor_slices(stringTensor)\n",
    "# Now map the new dataset\n",
    "dataset = stringDataset.map(\n",
    "    datasetMapper, num_parallel_calls=TensorFlow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Test sample\n",
    "for image1, image2, label in dataset.take(1):\n",
    "    print(\"Image 1 =\", image1)\n",
    "    print(\"Image 2 =\", image2)\n",
    "    print(\"Label =\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f4ccf",
   "metadata": {},
   "source": [
    "## 2.4 ROC Plotting\n",
    "We could use `sklearn.metrics.roc_curve` to plot the ROC, except that the function only accepts two vectors - one for the actual values and the other for the predicted values. Getting the actual value is straightforward but not so for the predicted values since the images need to be run through the model.\n",
    "\n",
    "Also, the distance measured is Euclidean distance. It has the potential to be between zero and infinity however, which is incompatible with the ROC's 0-1 range. The tanH function can be used to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f295c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the lists for actual values and predicted (loss) values\n",
    "actualLabelList = []\n",
    "predictedLabelList = []\n",
    "\n",
    "# Split the dataset into three single value ones\n",
    "leftImageDataset = dataset.map(lambda x, y, z: x, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "rightImageDataset = dataset.map(lambda x, y, z: y, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "actualLabels = dataset.map(lambda x, y, z: z, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "\n",
    "# Batch the first two datasets\n",
    "leftImageDataset = leftImageDataset.batch(BATCH_SIZE)\n",
    "rightImageDataset = rightImageDataset.batch(BATCH_SIZE)\n",
    "\n",
    "# Get the model and its outputs\n",
    "kerasModel = smlb.get_compiled_model()\n",
    "leftEmbeddings = kerasModel.predict(leftImageDataset)\n",
    "rightEmbeddings = kerasModel.predict(rightImageDataset)\n",
    "\n",
    "# Calculate the difference\n",
    "import math\n",
    "embeddingDifferences = leftEmbeddings - rightEmbeddings\n",
    "for embeddingDifference in embeddingDifferences:\n",
    "    distance = NumPy.square(embeddingDifference).sum()\n",
    "    predictedLabelList.append(math.tanh(distance))\n",
    "\n",
    "for actualLabel in actualLabels:\n",
    "    actualLabelList.append(actualLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e78ed6e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics # this must be imported explicitly\n",
    "falsePositives, truePositives, others = sklearn.metrics.roc_curve(actualLabelList, predictedLabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd7afce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PyPlot.plot(100*falsePositives, 100*truePositives)\n",
    "PyPlot.xlabel('False positives [%]')\n",
    "PyPlot.ylabel('True positives [%]')\n",
    "PyPlot.grid(True)\n",
    "ax = PyPlot.gca()\n",
    "ax.set_aspect('equal')\n",
    "PyPlot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb683be4",
   "metadata": {},
   "source": [
    "The model is *effectively random* right now, but we are focusing to at least have *a* model to work with.\n",
    "\n",
    "(the following are temp codes and should be skipped)\n",
    "```python\n",
    "# Split the dataset into three 1D ones\n",
    "leftImageDataset = dataset.map(lambda x, y, z: x, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "rightImageDataset = dataset.map(lambda x, y, z: y, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "actualLabels = dataset.map(lambda x, y, z: z, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "\n",
    "# Create the model's outputs from the first two datasets\n",
    "leftEmbeddings = smlb.predict(leftImageDataset)\n",
    "rightEmbeddings = smlb.predict(rightImageDataset)\n",
    "\n",
    "# Use triplet loss to fill in predictedLabels\n",
    "semiHardLoss = TensorFlowAddons.losses.TripletSemiHardLoss()\n",
    "for leftEmbedding, rightEmbedding in zip(leftEmbeddings, rightEmbeddings):\n",
    "    predictedLabelList.append(semiHardLoss(leftEmbedding, rightEmbedding))\n",
    "\n",
    "for actualLabel in actualLabels:\n",
    "    actualLabelList.append(actualLabel)\n",
    "```\n",
    "```python\n",
    "semiHardLoss = TensorFlowAddons.losses.TripletSemiHardLoss()\n",
    "kerasModel = smlb.get_compiled_model()\n",
    "for image1, image2, actualLabel in dataset:\n",
    "    embedding1 = kerasModel(image1)\n",
    "    embedding2 = kerasModel(image2)\n",
    "    actualLabelList.append(actualLabel)\n",
    "    predictedLabelList.append(semiHardLoss(embedding1, embedding2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f35ce",
   "metadata": {},
   "source": [
    "# 4. Auxillary Functions\n",
    "Sorry, let me rephrase, I will add some auxiliary functions:\n",
    "- to check if a given image is in the database, and returns the closest image filename or \"\"\n",
    "- to add an image to the database\n",
    "\n",
    "the GUI can then call these functions and get their results.\n",
    "## 4.1 Image Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "439bc4e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-19-b5234d787a97>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-b5234d787a97>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    return {}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "_image_database = {}\n",
    "\n",
    "def _get_embedding_distance(embedding1, embedding2) -> float:\n",
    "    embeddingDifference = embedding1 - embedding2\n",
    "    return math.tanh(NumPy.square(embeddingDifference).sum())\n",
    "\n",
    "def _load_image_database(smlb:SimpleMLBuilder) -> dict:\n",
    "    database = {}\n",
    "    for currentDirectory, directoryNames, fileNames in os.walk(DATABASE_DIR):\n",
    "        for fileName in fileNames:\n",
    "    return {}\n",
    "\n",
    "def _get_image_database(smlb:SimpleMLBuilder) -> dict:\n",
    "    if _image_database.empty():\n",
    "        _load_image_database(smlb)\n",
    "    \n",
    "    return _image_database\n",
    "\n",
    "def add_image_to_database(imageJpegFilePath:str):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def get_closest_image(imageJpegFilePath:str, thresholdDistance:float) -> str:\n",
    "    smlb = SimpleMLBuilder()\n",
    "    smlb.load(\"Model 1\")\n",
    "    \n",
    "    # Load the image file\n",
    "    imageBinary = TensorFlow.io.read_file(imageJpegFilePath)\n",
    "    # Decode the image file\n",
    "    imageRaw = TensorFlow.io.decode_jpeg(imageBinary, channels=3)\n",
    "    # Resize the raw image\n",
    "    imageRawResized = TensorFlow.image.resize(\n",
    "        imageRaw, IMAGE_SIZE, method=TensorFlow.image.ResizeMethod.BICUBIC\n",
    "    )\n",
    "    \n",
    "    kerasModel = smlb.get_compiled_model()\n",
    "    compEmbedding = kerasModel(imageRawResized)\n",
    "    bestName = \"\"\n",
    "    bestDistance = 1\n",
    "    for name, embedding in _get_image_database(smlb):\n",
    "        embeddingDistance = get_embedding_distance(embedding, compEmbedding)\n",
    "        if embeddingDistance < bestDistance:\n",
    "            bestName = name\n",
    "            bestDistance = embeddingDistance\n",
    "    \n",
    "    if bestDistance < thresholdDistance:\n",
    "        return bestName\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2948a",
   "metadata": {},
   "source": [
    "# 5. Model Improving\n",
    "The model is in desperate need of improvement.\n",
    "## 5.1 Testing Automation\n",
    "The following code was made to automate the process of testing. It is a combination and slight modification of the codes for sections 2.3 and 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968a8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "import os\n",
    "import sklearn.metrics\n",
    "stringList = []\n",
    "with open(TESTING_PAIRS_TXT) as fileHandle:\n",
    "    for line in fileHandle:\n",
    "        stringList.append(line.split())\n",
    "\n",
    "# Convert to Tensor\n",
    "stringTensor = TensorFlow.constant(stringList, TensorFlow.dtypes.string)\n",
    "\n",
    "stringZeroTensor = TensorFlow.constant('0', TensorFlow.dtypes.string)\n",
    "floatZeroTensor = TensorFlow.constant(0, TensorFlow.dtypes.float32)\n",
    "floatOneTensor = TensorFlow.constant(1, TensorFlow.dtypes.float32)\n",
    "\n",
    "def getImageTensorByAbsolutePath(absolutePath:TensorFlow.Tensor)\\\n",
    "-> TensorFlow.Tensor:\n",
    "    # Load the image via TensorFlow.io.read_file\n",
    "    imageBinary = TensorFlow.io.read_file(absolutePath)\n",
    "    # Decode the binary via TensorFlow.io.decode_jpeg\n",
    "    imageRaw = TensorFlow.io.decode_jpeg(imageBinary, channels=3)\n",
    "    # Resize the raw image in case that the image is not 64x64 for some reason\n",
    "    imageRawResized = TensorFlow.image.resize(\n",
    "        imageRaw, IMAGE_SIZE, method=TensorFlow.image.ResizeMethod.BICUBIC\n",
    "    )\n",
    "    return imageRawResized\n",
    "\n",
    "def datasetMapper(stringTensor:TensorFlow.Tensor) -> tuple:\n",
    "    # Start with the first image\n",
    "    # First, get the relative path\n",
    "    relativePath = stringTensor[0]\n",
    "    # We need to have an absolute path, not a relative one, so convert it\n",
    "    absolutePath = TensorFlow.strings.join([TESTING_PAIRS_DIR, os.path.sep, relativePath])\n",
    "    # Then, load the image\n",
    "    image1 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Now do it again for image 2\n",
    "    relativePath = stringTensor[1]\n",
    "    absolutePath = TensorFlow.strings.join([TESTING_PAIRS_DIR, os.path.sep, relativePath])\n",
    "    image2 = getImageTensorByAbsolutePath(absolutePath)\n",
    "    \n",
    "    # Lastly, the label should be a float, this can be done by checking if\n",
    "    # the tensor value is zero or not\n",
    "    parity = TensorFlow.cond(\n",
    "        stringTensor[2] == stringZeroTensor,\n",
    "        lambda: floatZeroTensor,\n",
    "        lambda: floatOneTensor\n",
    "    )\n",
    "    \n",
    "    # Images and label are now good, return them\n",
    "    return image1, image2, parity\n",
    "\n",
    "# Turn stringTensor into a dataset first\n",
    "stringDataset = TensorFlow.data.Dataset.from_tensor_slices(stringTensor)\n",
    "# Now map the new dataset\n",
    "dataset = stringDataset.map(\n",
    "    datasetMapper, num_parallel_calls=TensorFlow.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Split the dataset into three single value ones\n",
    "leftImageDataset = dataset.map(lambda x, y, z: x, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "rightImageDataset = dataset.map(lambda x, y, z: y, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "actualLabels = dataset.map(lambda x, y, z: z, num_parallel_calls=TensorFlow.data.AUTOTUNE)\n",
    "\n",
    "# Batch the first two datasets\n",
    "leftImageDataset = leftImageDataset.batch(BATCH_SIZE)\n",
    "rightImageDataset = rightImageDataset.batch(BATCH_SIZE)\n",
    "\n",
    "# Create the list of actual values\n",
    "actualLabelList = []\n",
    "for actualLabel in actualLabels:\n",
    "    actualLabelList.append(actualLabel)\n",
    "\n",
    "def test_model(smlb:SimpleMLBuilder):\n",
    "    # Get the model and its outputs\n",
    "    kerasModel = smlb.get_compiled_model()\n",
    "    leftEmbeddings = kerasModel.predict(leftImageDataset)\n",
    "    rightEmbeddings = kerasModel.predict(rightImageDataset)\n",
    "    \n",
    "    # Calculate the differences\n",
    "    predictedLabelList = []\n",
    "    embeddingDifferences = leftEmbeddings - rightEmbeddings\n",
    "    for embeddingDifference in embeddingDifferences:\n",
    "        distance = NumPy.square(embeddingDifference).sum()\n",
    "        predictedLabelList.append(math.tanh(distance))\n",
    "    \n",
    "    falsePositives, truePositives, others = sklearn.metrics.roc_curve(actualLabelList, predictedLabelList)\n",
    "    PyPlot.plot(100*falsePositives, 100*truePositives)\n",
    "    PyPlot.xlabel('False positives [%]')\n",
    "    PyPlot.ylabel('True positives [%]')\n",
    "    PyPlot.grid(True)\n",
    "    ax = PyPlot.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    PyPlot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8d05ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(smlb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceeb984",
   "metadata": {},
   "source": [
    "## 4.2 Recreating the Other Datasets\n",
    "It appears that the datasets have a chance to stop working properly here. They have to be recreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e74ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 200 classes.\n",
      "Using 4500 files for training.\n",
      "Found 5000 files belonging to 200 classes.\n",
      "Using 500 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed = random.randrange(0, 16777216)\n",
    "trainingDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"training\", interpolation=\"bicubic\"\n",
    ")\n",
    "validationDataset = Keras.utils.image_dataset_from_directory(\n",
    "    TRAINING_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, seed=seed,\n",
    "    validation_split=VALIDATION_SPLIT, subset=\"validation\", interpolation=\"bicubic\"\n",
    ")\n",
    "trainingOptimizedDataset = trainingDataset.cache().shuffle(1000).prefetch(buffer_size=TensorFlow.data.AUTOTUNE)\n",
    "validationOptimizedDataset = validationDataset.cache().prefetch(buffer_size=TensorFlow.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a98ce",
   "metadata": {},
   "source": [
    "## 4.3 Improving the Model\n",
    "An improved model architecture is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db033886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMLB] Fully initialized!\n",
      "[SMLB] Training X values have been set!\n",
      "[SMLB] Validation dataset has been set!\n",
      "[SMLB] Layering started. Added layer: {'type': 'Input', 'shape': (64, 64, 3)}\n",
      "[SMLB] Added layer: {'type': 'Rescaling', 'scale': 0.00392156862745098, 'offset': 0.0}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Dropout', 'rate': 0.2}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Dropout', 'rate': 0.2}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 64, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Dropout', 'rate': 0.2}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Dropout', 'rate': 0.2}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 128, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'MaxPool2D', 'pool_size': (2, 2), 'strides': (2, 2)}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same'}\n",
      "[SMLB] Modified layer: {'type': 'Conv2D', 'filters': 256, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'same', 'activation': 'relu'}\n",
      "[SMLB] Added layer: {'type': 'Flatten'}\n",
      "[SMLB] Added layer: {'type': 'Dense', 'units': 256}\n",
      "[SMLB] Modified layer: {'type': 'Dense', 'units': 256, 'kernel_regularizer': <keras.regularizers.L2 object at 0x000002284C1085B0>}\n",
      "[SMLB] Set loss function: <tensorflow_addons.losses.triplet.TripletSemiHardLoss object at 0x000002284C108370>\n",
      "[SMLB] Building model with learning rate = 0.0001...\n",
      "[SMLB] Model built! Details:\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 32, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               16777472  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,697,472\n",
      "Trainable params: 18,697,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "smlb = SimpleMLBuilder(True)\n",
    "smlb.set_training_features(trainingOptimizedDataset)\n",
    "smlb.set_validation_dataset(validationOptimizedDataset)\n",
    "\n",
    "smlb.start_layering((64, 64, 3))\n",
    "smlb.add_rescaling_layer()\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_dropout_layer(0.2)\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_dropout_layer(0.2)\n",
    "smlb.add_convolution_layer(64, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_dropout_layer(0.2)\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_dropout_layer(0.2)\n",
    "smlb.add_convolution_layer(128, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_pooling_layer(\"max\", (2,2), (2,2))\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_convolution_layer(256, (3,3), pad=True)\n",
    "smlb.add_activation_layer(\"relu\")\n",
    "smlb.add_flatten_layer()\n",
    "smlb.add_dense_layer(256)\n",
    "smlb.add_regularization_sublayer(\"l2\")\n",
    "\n",
    "smlb.set_custom_loss_function(TensorFlowAddons.losses.TripletSemiHardLoss())\n",
    "smlb.build(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c11bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 13s 273ms/step - loss: 0.9410 - val_loss: 0.8657\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 12s 273ms/step - loss: 0.3996 - val_loss: 0.7049\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 12s 275ms/step - loss: 0.2744 - val_loss: 0.6295\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 12s 273ms/step - loss: 0.2318 - val_loss: 0.4857\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 12s 272ms/step - loss: 0.2828 - val_loss: 0.5774\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 12s 271ms/step - loss: 0.1966 - val_loss: 0.5951\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 12s 263ms/step - loss: 0.1249 - val_loss: 0.2163\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 12s 269ms/step - loss: 0.1573 - val_loss: 0.2322\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 13s 285ms/step - loss: 0.1315 - val_loss: 0.1982\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 12s 262ms/step - loss: 0.1164 - val_loss: 0.2184\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 12s 271ms/step - loss: 0.0871 - val_loss: 0.2910\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 12s 271ms/step - loss: 0.0937 - val_loss: 0.1702\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 12s 270ms/step - loss: 0.0886 - val_loss: 0.2428\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 12s 271ms/step - loss: 0.0721 - val_loss: 0.1390\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 12s 273ms/step - loss: 0.1087 - val_loss: 0.1694\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 12s 268ms/step - loss: 0.1079 - val_loss: 0.2577\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 12s 272ms/step - loss: 0.1176 - val_loss: 0.1734\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 12s 264ms/step - loss: 0.1091 - val_loss: 0.1634\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 12s 272ms/step - loss: 0.1070 - val_loss: 0.1808\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 12s 275ms/step - loss: 0.0905 - val_loss: 0.1647\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 12s 267ms/step - loss: 0.0913 - val_loss: 0.1599\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 12s 268ms/step - loss: 0.0766 - val_loss: 0.1282\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 12s 272ms/step - loss: 0.0812 - val_loss: 0.1984\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 12s 276ms/step - loss: 0.0951 - val_loss: 0.1957\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 12s 273ms/step - loss: 0.0993 - val_loss: 0.2265\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 12s 272ms/step - loss: 0.1169 - val_loss: 0.2746\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 12s 273ms/step - loss: 0.1135 - val_loss: 0.2144\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 12s 270ms/step - loss: 0.0993 - val_loss: 0.2313\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 12s 274ms/step - loss: 0.1021 - val_loss: 0.2400\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 12s 276ms/step - loss: 0.0916 - val_loss: 0.2927\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 12s 269ms/step - loss: 0.1004 - val_loss: 0.3290\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 12s 269ms/step - loss: 0.0993 - val_loss: 0.2648\n",
      "[SMLB] ======== TRAINING DONE ========\n"
     ]
    }
   ],
   "source": [
    "smlb.run(100, earlyStop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81a9146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(smlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d398c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
